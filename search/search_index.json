{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Amazon messaging studies","text":"Documentation Updates <p>Author: Jerome Boyer</p> <p>Creation date: September 2023 - Updated 1/19/2024</p> <ul> <li>11/23: Add IBM MQ one-way point to point JMS implementation.</li> <li>12/23: Address an asynchronous implementation for S3 event processing</li> <li>1/24: Update SQS</li> </ul> <p>This repository includes a set of demonstrations and studies about messaging services in AWS or as Open Source. The services in scope are: Amazon MQ, Amazon SQS, SNS, MSK and Kinesis. </p> <p>The goal is to group personal notes and knowledge gathering around the different technologies, keep references to valuable sources of information, and do some hands-on work to be able to go deeper on some concepts. This content is as-is, does not represent my employer point of view, and can be shared as an open source. It may be useful for developers or solution architects.</p>"},{"location":"#audience","title":"Audience","text":"<p>The main readers of this website are people interested in AWS messaging and open-source messaging systems.</p> <ul> <li>Solution architects</li> <li>Developers, new to distributed system solutions, with interest in AWS and messaging systems</li> </ul> <p>This site is a companion of my Event-Driven Architecture book.</p>"},{"location":"#link-back-to-event-driven-architecture","title":"Link back to Event-Driven Architecture","text":"<p>The EDA reference architecture, as introduced in the following figure, uses the concept of event backbone, which is a messaging component supporting asynchronous communication between components</p> <p></p> <p>When zooming into this component, we can see queueing systems are very important to support event or message driven integration:</p> <p></p> <p>This repository tries to go deeper in each of those components.</p>"},{"location":"#amazon-mq","title":"Amazon MQ","text":"<p>Amazon MQ is a managed message broker for RabbitMQ or ActiveMQ. It runs on EC2 servers, and supports multi-AZs deployment with failover.</p> <p>The Amazon MQ workshops contain a lot of very good information to get started with Amazon MQ. This repository addresses some of the specific subjects not covered in detail in those workshops: JMS programming model, clear separation between producer and consumer applications, deeper dive to the open source versions, starting developing locally using docker, failover testing... Addressing unit testing, integration test, reactive messaging and reactive programming with queueing systems.</p> <p>The code will address the standard enterprise integration patterns of one-way point-to-point and request/responce point-to-point. But it may add more in the future.</p> <p>For the IaC, we will use AWS CDK as much as possible, with different stacks to be able to reuse common infrastructure like VPC, IAM roles, and an optional Cloud9 environment, and finally the broker configuration.</p> <p>Read more on Active MQ &gt;&gt;&gt;</p>"},{"location":"#aws-samples","title":"AWS Samples","text":"<p>AWS Sample git account includes samples for Amazon MQ, which can be used for inspiration:</p> <ul> <li>Github amazon-mq-workshop</li> <li>Github Amazon MQ Network of Brokers samples</li> </ul>"},{"location":"#rabbit-mq","title":"Rabbit MQ","text":"<p>The other Open Source engine using in Amazon MQ.</p> <p>To Be done.</p> How to connect to Rabbit MQ from different vpc or from on-premises? <p>This Creating static custom domain endpoints with Amazon MQ for RabbitMQ blog presents SSL and DNS resolution to access an NLB to facade brokers. Also the NLB can be used cross VPCs that are peered. Need NLB for broker specific TCP based protocol. Security group in the broker specify inbound traffic from the NLB only. NLB target group uses the broker static VPC endpoint address. NLB can also restrict who can acccess it.</p>"},{"location":"#solutions-in-this-repository","title":"Solutions in this repository","text":"<p>Some concrete code samples are part of this repository to demonstrate some of the important concepts or technology:</p> <ul> <li>Request-ReplyTo order ochestrator and participant based on JMS - ActiveMQ Classic release</li> <li>One Way Point-to-Point JMS based producer and consumer - ActiveMQ</li> <li>Request-Response for an order ochestrator and an order process participant based on JMS - ActiveMQ Artemis release</li> <li>AMQP Quarkus app point to point - ActiveMQ Artemis release</li> <li>Infrastructure as Code - VPC stack</li> <li>Infrastructure as Code - ActiveMQ active/standby topology stack</li> <li>Process S3 events for multi-tenant bucket with EDA</li> <li>IBM MQ JMS one-way point to point solution</li> </ul>"},{"location":"#messaging-related-sources-of-information","title":"Messaging related sources of information","text":"<ul> <li>re:Invent 2018 - Choosing the right messaging services.</li> <li>Amazon MQ RabbitMQ workshop</li> <li>Implementing enterprise integration patterns with AWS messaging services: point-to-point channels.</li> </ul>"},{"location":"activemq/","title":"Active MQ","text":"<p>This section is a quick summary from ActiveMQ Artemis version product documentation, ActiveMQ classic documentation and Amazon MQ ActiveMQ engine documentation for Active MQ 5.17.3 deployment as a managed service.</p>"},{"location":"activemq/#the-open-source-project","title":"The Open Source project","text":"<p>Active MQ is an Open Source software, multi-protocol, java based message broker. ActiveMQ has two main version of the product Active MQ 5.x (or classic) and Artemis 2.x which supports Jakarta Messsaging 3.1. It also supports embedding the broker in a java app.</p> <p>It supports message load balancing, HA. Multiple connected \"master\" brokers can dynamically respond to consumer demand by moving messages between the nodes in the background.</p> <p>Amazon MQ - Active MQ engine supports the Classic version.</p> <p>Active MQ supports different messaging patterns: queue and topic:</p> <ul> <li>Queue supports point to point, and request/replyTo pattern.</li> <li>Queue can have multiple senders and consumers, the message will be load balanced between consumers. Messages acknowledged are removed from the queue.</li> <li> <p>Message with a TTL will be removed from queue, without being consumed.</p> </li> <li> <p>Topic supports pub/sub.</p> </li> <li>With Topic, receiver starts to receive only the new messages, that are being sent by the sender. Messages sent to topic without consumer are lost.</li> <li>Topics support the fan-out pattern. All Messages sent by any senders are received by all connected receivers.</li> </ul>"},{"location":"activemq/#value-propositions","title":"Value propositions","text":"<ul> <li>Java 11+, JMS 2.0, Jakarta Messaging 3.0</li> <li>Protocols supported: STOMP, AMQP, OpenWire, MQTT, NMS (.Net), CMS (C++),  HornetQ, core Artemis API.</li> <li>Support Queues and Topics for pub/sub</li> <li>Performance with message persistence.</li> <li>Integrated with Java EE application server or embbeded in a java app, or standalone using lightweight netty server.</li> <li>HA solution with automatic client failover</li> <li>Flexible clustering</li> <li>Messages can be ordered by message group</li> <li>Message filtering using selectors to perform content based routing</li> <li>Unlimited message size so there is not need to plan for unexpected messages</li> <li>Message Delay and scheduling</li> <li>Distribute transactions to manage complex multi stage transactions such as database access</li> <li>Virtual Topics and composite destinations</li> <li>Complex redelivery policy</li> </ul>"},{"location":"activemq/#configurations","title":"Configurations","text":"<p>A configuration contains all of the settings for the ActiveMQ brokers, in XML format. See the product documentation to get what can be defined.</p> <p>The configuration parts to consider for any deployment:</p> <ul> <li>What transport connector to enable for which protocol (amqp, openwire, ...).</li> <li>What persistence to use as backend to save messages until read. The default persistence mechanism is the KahaDB store.</li> <li>The need to expose JMX</li> <li>Control Flow for back preassure management in case of slow consumers. Persistence time.</li> </ul> <p>It is possible to configure users and groups, and then the <code>authorizationMap</code> so a specific queue or topic can only be accessed by a specific user/app (The declaration below, allows user1 to manage, write and read from <code>queue.user1</code>, but not user2, who is allowed admin, read and write on <code>topic.user2</code>): </p> <pre><code>&lt;authorizationPlugin&gt;\n    &lt;map&gt;\n    &lt;authorizationMap&gt;\n        &lt;authorizationEntries&gt;\n          &lt;authorizationEntry admin=\"admin,activemq-webconsole\" queue=\"&amp;gt;\" read=\"admin,activemq-webconsole\" write=\"admin,activemq-webconsole\"/&gt;\n          &lt;authorizationEntry admin=\"admin,activemq-webconsole\" topic=\"&amp;gt;\" read=\"admin,activemq-webconsole\" write=\"admin,activemq-webconsole\"/&gt;\n          &lt;authorizationEntry admin=\"admin,user1\" queue=\"queue.user1\" read=\"user1\" write=\"user1\"/&gt;\n          &lt;authorizationEntry admin=\"admin,user2\" read=\"user2\" topic=\"topic.user2\" write=\"user2\"/&gt;\n          &lt;authorizationEntry admin=\"admin,user1,user2\" read=\"admin,user1,user2\" topic=\"ActiveMQ.Advisory.&amp;gt;\" write=\"admin,user1,user2\"/&gt;\n        &lt;/authorizationEntries&gt;\n        &lt;tempDestinationAuthorizationEntry&gt;\n        &lt;tempDestinationAuthorizationEntry admin=\"tempDestinationAdmins\" read=\"tempDestinationAdmins\" write=\"tempDestinationAdmins\"/&gt;\n        &lt;/tempDestinationAuthorizationEntry&gt;\n    &lt;/authorizationMap&gt;\n    &lt;/map&gt;\n&lt;/authorizationPlugin&gt;\n</code></pre> <p>In order to apply the modifications done to the broker configuration, the broker must be rebooted. A reboot can be scheduled, and use specific configuration revision to specify which configuration updates to apply.</p>"},{"location":"activemq/#monitoring","title":"Monitoring","text":"<p>Most of the management and monitoring is done via the Console or the JMX MBeans. See the monitoring Lab for JMX local management, and Amazon MQ monitoring.</p>"},{"location":"activemq/#active-mq-topologies","title":"Active MQ Topologies","text":"<p>The Active product documentation HA chapter gives all the details on the different topologies supported. Here are the important points to remember:</p> <ul> <li>Use Live/backup node groups when more than two brokers are used.</li> <li>A backup server is owned by only one live server.</li> <li>Two strategies for backing up a server shared store and replication.</li> <li> <p>When using a shared store, both live and backup servers share the same entire data directory using a shared file system (SAN).</p> <p></p> <p>Figure 1: Active/standby shared storage</p> </li> <li> <p>Only persistent message data will survive failover.</p> </li> <li> <p>With replication the data filesystem is not shared, but replicated from live to standby.  At start-up the backup server will first need to synchronize all existing data from the live server, which brings lag. This could be minimized.</p> <p></p> <p>Figure 2: Active/standby replicate storage</p> </li> <li> <p>With replicas when live broker restarts and failbacks, it will replicate data from the backup broker with the most fresh messages.</p> </li> <li>Brokers with replication are part of a cluster. So <code>broker.xml</code> needs to include cluster connection. Live | backup brokers are in the same node-group.</li> </ul>"},{"location":"activemq/#mesh","title":"Mesh","text":"<p>We can choose a network of brokers with multiple active/standby brokers, like a broker Mesh. This topology is used to increase the number of client applications. Any one of the two Active/Stanby brokers can be active at a time with messages stored in a shared durable storage. There is no single point of failure as in client/server or hub and spoke topologies. A client can failover another broker improving high availability. </p> <p>The following diagram illustrates a configuration over 3 AZs, and the corresponding CloudFormation template can be found here.</p> <p></p> <p>Figure 3: Active MQ mesh cluster deployment</p> <p>Each broker can accept connections from clients. The client endpoints are named <code>TransportConnectors</code>. Any client connecting to a broker uses a failover string that defines each broker that the client can connect to send or receive messages.</p> <pre><code>amqp+ssl://b-5......87c1e-1.mq.us-west-2.amazonaws.com:5671\namqp+ssl://b-5......87c1e-2.mq.us-west-2.amazonaws.com:5671\n</code></pre> <p>In order to scale, client connections can be divided across brokers. </p> <p>Because those brokers are all connected using network connectors, when a producer sends messages to say NoB1, the messages can be consumed from NoB2 or from NoB3. This is because <code>conduitSubscriptions</code> is set to false.</p> <p>Essentially we send messages to any brokers, and the messages can still be read from a different brokers.</p> <p>Brokers are connected with each other using <code>OpenWire</code> network connectors. Within each broker configuration, for each queue and topic, there are a set of <code>networkConnector</code> items defining connection from the current broker and to the two other brokers in the mesh. So each broker has a different networkConnector, to pair to each other broker.</p> <pre><code>  &lt;networkConnectors&gt;\n    &lt;networkConnector conduitSubscriptions=\"false\" consumerTTL=\"1\" messageTTL=\"-1\" name=\"QueueConnector_ConnectingBroker_1_To_2\" uri=\"masterslave:(ssl://b-c2....2-1.mq.us-west-2.amazonaws.com:61617,ssl://b-c2...2-2.mq.us-west-2.amazonaws.com:61617)\" userName=\"mqadmin\"&gt;\n      &lt;excludedDestinations&gt;\n        &lt;topic physicalName=\"&amp;gt;\"/&gt;\n      &lt;/excludedDestinations&gt;\n    &lt;/networkConnector&gt;\n    &lt;networkConnector conduitSubscriptions=\"false\" consumerTTL=\"1\" messageTTL=\"-1\" name=\"QueueConnector_ConnectingBroker_1_To_3\" uri=\"masterslave:(ssl://b-ad...647-1.mq.us-west-2.amazonaws.com:61617,ssl://b-ad...d747-2.mq.us-west-2.amazonaws.com:61617)\" userName=\"mqadmin\"&gt;\n      &lt;excludedDestinations&gt;\n        &lt;topic physicalName=\"&amp;gt;\"/&gt;\n      &lt;/excludedDestinations&gt;\n    &lt;/networkConnector&gt;\n</code></pre> <p>The messages do not flow to other brokers if no consumer is available.</p> <p>The duplex attribute on <code>networkConnector</code> essentially establishes a two-way connection on the same port. This would be useful when network connections are traversing a firewall and is common in Hub and Spoke broker topology. In a Mesh topology, it is recommended to use explicit unidirectional networkConnector as it allows flexibility to include or exclude destinations.</p> <p>Because these brokers are all connected using network connectors, when a producer sends messages to say NoB1, the messages can be consumed from NoB2 or from NoB3.</p>"},{"location":"activemq/#hub-and-spoke","title":"Hub and Spoke","text":"<p>For Hub and Spoke a central broker dispatches to other connected broker.</p> <p></p> <p>Figure 4: Hub - spoke topology</p>"},{"location":"activemq/#connection-from-client-app","title":"Connection from client app","text":"<p>Once deployed there are 5 differents end points to support the different protocols:</p> <ul> <li>OpenWire \u2013 ssl://xxxxxxx.xxx.com:61617</li> <li>AMQP \u2013 amqp+ssl:// xxxxxxx.xxx.com:5671</li> <li>STOMP \u2013 stomp+ssl:// xxxxxxx.xxx.com:61614</li> <li>MQTT \u2013 mqtt+ssl:// xxxxxxx.xxx.com:8883</li> <li>WSS \u2013 wss:// xxxxxxx.xxx.com:61619</li> </ul> <p>As of Dec 2023, Amazon MQ doesn't support Mutual Transport Layer Security (mTLS) authentication.</p> <p>In active/standby deployment, any one of the brokers can be active at a time. Any client connecting to a broker uses a failover string that defines each broker that the client can connect to.</p> <pre><code>failover:(ssl://b-9f..7ac-1.mq.eu-west-2.amazonaws.com:61617,ssl://b-9f...c-2.mq.eu-west-2.amazonaws.com:61617)\n</code></pre> <p>Adding failover in broker url ensures that whenever server goes up, it will reconnect it immediately. See Active MQ documentation on failover</p> <p>When the active broker reboots, the client applications may report issue but reconnect to the backup broker. Below is an example of logs:</p> <pre><code>Transport: ssl://b-d....-2.mq.us-west-2.amazonaws.com/10.42.0.113:61617] WARN org.apache.activemq.transport.failover.FailoverTransport - Transport (ssl://b-d...-2.mq.us-west-2.amazonaws.com:61617) failed , attempting to automatically reconnect: {}\njava.io.EOFException\n        at java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\n    ...\n\n[ActiveMQ Task-3] INFO org.apache.activemq.transport.failover.FailoverTransport - Successfully reconnected to ssl://b-d...-1.mq.us-west-2.amazonaws.com:61617\n</code></pre> <p>In the context of cluster mesh, each application may use different failover URL to connect to different brokers.</p> <p>One sender can have the following URL configuration:</p> <pre><code>failover:(ssl://b-650....e-1.mq.us-west-2.amazonaws.com:61617,ssl://b-650...e-2.mq.us-west-2.amazonaws.com:61617)\n</code></pre> <p>and one consumer with the url:</p> <pre><code>failover:(ssl://b-9f69...f-1.mq.us-west-2.amazonaws.com:61617,ssl://b-9f69...f-2.mq.us-west-2.amazonaws.com:61617)\n</code></pre> <p>The networkConnector in each broker configuration links each broker per pair, and messages flow between brokers using <code>networkConnectors</code> only when a consumer demands them. The messages do not flow to other brokers if no consumer is available.</p>"},{"location":"activemq/#storage","title":"Storage","text":"<p>The ActiveMQ message storage is an embeddable transactional message storage solution. It uses a transaction journal to support recovery. Messages are persisted in data logs (up to 32mb size) with reference to file location saved in KahaDB, in memory. Messages are in memory and then periodically inserted in the storage in the frequency of <code>checkpointInterval</code> ms. Version 5.14.0 introduces journal synch to disk strategy: <code>always</code> ensures every journal write is followed by a disk sync (JMS durability requirement). </p> <p>Message data logs includes messages/acks and transactional boundaries. Be sure to have the individual file size greater than the expected largest message size.</p> <p>Also broker who starts to have memory issue, will throttle the producer or even block it. See this Producer flow control article for deeper explanation and configuration per queue.</p> <p>Messages can be archived into separate logs.</p> <p>See the product documentation for persistence configuration.</p>"},{"location":"activemq/#faqs","title":"FAQs","text":"How to create queue or resources? <p>With open source Active MQ, we can use JMS API as they can be created dynamically via code, or use JMX. Static definitions can be done in the broker.xml file:</p> What is the advantage of replicas vs shared storage? <p>Shared storage needs to get SAN replication to ensure DR at the storage level. If not the broker file system is a single point of failure. It adds cost to the solution but it performs better. Replicas is ActiveMQ integrate solution to ensure High availability and sharing data between brokers. Slave broker copies data from Master. States of the brokers are not exchanged with replicas, only messages are. For Classic, JDBC message store could be used. Database replication is then used for DR. When non durable queue or topic are networked, with failure, inflight messages may be lost.</p> What is the difference between URL failover and implementing an ExceptionListener? <p>Java Messaging Service  has no specification on failover for JMS provider. When broker fails, there will be a connection Exception. The way to manage this exception is to use the asynchronous <code>ExceptionListener</code> interface which will give developer maximum control over when to reconnect, assessing what type of JMS error to better act on the error. ActiveMQ offers the failover transport protocol, is for connection failure, and let the client app to reconnect to another broker as part of the URL declaration. Sending message to the broker will be blocked until the connection is restored. Use <code>TransportListener</code> interface to understand what is happening. This is a good way to add logging to the application to report on the connection state.</p> When messages are moved to DLQ? <p>Producer app can set <code>setTimeToLive</code> with millisecond parameter. When the message has not being delivered to consumer, ActiveMQ move it to an expiry address, which could be mapped to a dead-letter queue. In fact a TTL set on a producer, will make ActiveMQ creating an <code>ActiveMQ.DLQ</code> queue. It is recommended to setup a DLQ per queue or per pair of request/response queues. ActiveMQ will never expire messages sent to the DLQ. See product documentation</p> <p><pre><code>&lt;policyEntry queue=\"order*\"&gt;\n    &lt;deadLetterStrategy&gt;\n        &lt;individualDeadLetterStrategy queuePrefix=\"DLQ.\" useQueueForQueueMessages=\"true\"/&gt;\n    &lt;/deadLetterStrategy&gt;\n&lt;/policyEntry&gt;\n</code></pre> Use the <code>&lt;deadLetterStrategy&gt; &lt;sharedDeadLetterStrategy processExpired=\"false\" /&gt;</code> to disable DLQ processing.</p> What is the constantPendingMessageLimitStrategy parameter? <p>When consumers are slow to process message from topic, and the broker is not persisting message, then messages in the RAM will impact consumer and producer performance. This parameter specifies how many messages to keep and let old messages being replace by new ones. See slow consumer section of the product documentation.</p> Broker clustering <p>Brokers in a cluster can share the message processing, each broker manages its own storage and connections. A core bridge is automatically created. When message arrives it will be send to one of the broker in a round-robin fashion. It can also distribute to brokers that have active consumers. There are different topologies supported: symmetric cluster where all nodes are connected to each other, or chain cluster where node is connected to two neighbores, . With a symmetric cluster each node knows about all the queues that exist on all the other nodes and what consumers they have.</p> Configuring Transport <p>Acceptor defines a way in which connections can be made to ActiveMQ broker. Here is one example:  <pre><code>  &lt;acceptor name=\"artemis\"&gt;tcp://172.19.0.2:61616?tcpSendBufferSize=1048576;tcpReceiveBufferSize=1048576;amqpMinLargeMessageSize=102400;protocols=CORE,AMQP,STOMP,HORNETQ,MQTT,OPENWIRE;useEpoll=true;amqpCredits=1000;amqpLowCredits=300;amqpDuplicateDetection=true;supportAdvisory=false;suppressInternalManagementObjects=false&lt;/acceptor&gt;\n</code></pre> Connectors define how to connect to the brokers, used when brokers are in cluster or bridged. When a client app, using ClientSessionFactory, uses indirectly connector.</p> What are the metrics to assess to decide to move to server mesh topology? <p>Server mesh is used to increase the number of consumers by adding brokers that may replicate messages. Broker's memory usage. Looking at the number of messages a specific consumer has acknowledged (inflight). Number of consumer per queue. Other important metrics are looking at queue attributes like size, DLQ content.</p> How to be quickly aware of broker is rebooting? <p>Create a CloudWatch alert on the EC2 rebooting event.</p> Why using Jolokia with Active MQ? <p>Some key reasons why developers use Jolokia for ActiveMQ:</p> <ul> <li>Jolokia allows easy monitoring and management of ActiveMQ brokers and queues/topics via HTTP/JSON. This is more convenient than JMX remoting.</li> <li>It provides remote access to JMX beans without the need to configure JMX ports/SSL etc.</li> <li>Jolokia converts JMX operations to JSON over HTTP. </li> <li>It allows bulk JMX operations to be performed with a single request. This improves performance compared to remote JMX.</li> <li>It can auto-discover brokers and provide an aggregated view of multiple ActiveMQ instances.</li> <li>There are Jolokia client libraries and tools available for Java, JavaScript, Go etc which simplify working with ActiveMQ via Jolokia.</li> <li>Jolokia is not tied to ActiveMQ specifically and can work across different JMX-enabled applications. This makes it reusable.</li> <li>Amazon MQ does not support Jolokia.</li> </ul>"},{"location":"activemq/#code-samples","title":"Code samples","text":"<ul> <li>Point to point producer to consumer using JMS running locally to start playing with Active MQ classic, or deploy the two apps and the Broker on Amazon MQ with AWS CDK as infrastructure as code.</li> </ul>"},{"location":"activemq/#to-address-in-the-future","title":"To address in the future","text":"<ul> <li>amqp client </li> <li>reactive messaging with brocker as channel</li> <li>stomp client</li> <li>openwire client</li> </ul>"},{"location":"amazonmq/","title":"Amazon MQ","text":"<p>Amazon MQ is a managed message broker for RabbitMQ or ActiveMQ. It runs on EC2 servers, and supports multi-AZs deployment with failover.</p> <p>We can create brokers via Console (see this lab), using the AWS CLI, SDK or CDK.</p> <p>As a queueing system, when a message is received and acknowledged by one receiver, it is no longer on the queue, and the next receiver to connect gets the next message on the queue.</p> <p>Multiple senders can send messages to the same queue, and multiple receivers can receive messages from the same queue. But each message is only delivered to one receiver only.</p> <p>With topics, a consumer gets messages from when it starts to consume, previous messages will not be seen. Multiple subscribers will get the same message. All the messages sent to the topic, from any sender, are delivered to all receivers.</p> <p>The Amazon MQ Configuration is a specific object to manage the configuration of the broker. It can be created before the broker and supports Active MQ activemq.xml configuration.</p>"},{"location":"amazonmq/#value-propositions","title":"Value propositions","text":"<ul> <li>Keep skill investment and code compatibility with existing on-premises applications.</li> <li>Reduce cost.</li> <li>Deployment automation with CloudFormation, deploy in minutes.</li> <li>Reduced operation overhead, including provisioning, updates, monitoring, maintenance, security and troubleshooting.</li> <li>Vertical scaling with EC2 instance size and type.</li> <li>Horizontal scaling through network of brokers.</li> <li>Queues and topics are in one service so we can easily fan out or build durable queue.</li> <li>Both Transient and persistent messages are supported to optimize for durability or performance.</li> <li>Lower latency.</li> <li>Support Lift and shift of existing apps to the cloud, or use an hybrid architecture.</li> </ul>"},{"location":"amazonmq/#performance-considerations","title":"Performance considerations","text":"<ul> <li>The size of the message determines performance of the broker, above 100k, storage throughput is a limiting factor.</li> <li>With in-memory queue without persistence, Active MQ can reach high throughput. With no message lost goal, it uses persistence with flush at each message, and it will be bound by the I/O capacity of the underlying persistence store, EBS or EFS. <code>mq.m5.large</code>.</li> <li>To improve throughput with Amazon MQ, make sure to have consumers processing messaging as fast as, or faster than the producers are pushing messages.</li> <li>With EFS replication, with cluster and HA, throughput will be lower.</li> <li>Blog: \"Measuring the throughput for Amazon MQ using the JMS Benchmark\". </li> </ul>"},{"location":"amazonmq/#pricing","title":"Pricing","text":"<p>We pay by the hour of broker time according to the type of EC2 used as broker. The topology also impacts pricing between single, active/standby and cluster.</p> <p>Storage price is based on GB persisted on EFS in case of cluster, or EBS in case of single instance.</p> <p>Data transfer pricing applies too:</p> <ul> <li>For Traffic forwarded between brokers across availability zones in the same region</li> <li>For Traffic cross-region based on EC2 pricing. In region is not charged.</li> <li>For traffic out to the internet.</li> </ul>"},{"location":"amazonmq/#monitoring","title":"Monitoring","text":"<p>AmazonMQ publishes utilization metrics for the brokers, such as <code>CpuUtilization, HeapUsage, NetworkOut</code>. If we have a configuration with a primary and a secondary broker, we will have independent metrics for each instance.</p> <p>It also publishes metrics for queues and Topics such as <code>MemoryUsage, EnqueueCount</code> (messages published by producers), <code>DispatchCount</code> (message delivered to consumers).</p> <p></p> <p>Figure 1: Queue monitoring with metrics</p> <p>Using Cloudwatch alarm we can auto scale the consumer based on metrics value.</p> <p>From the AWS Amazon MQ Broker console, we can access to the Active MQ console and see the queues.</p> <p></p> <p>Figure 2: Active MQ admin console</p>"},{"location":"amazonmq/#maintenance","title":"Maintenance","text":"<p>AWS is responsible of the hardware, OS, engine software update. Maintenance may be scheduled once a week and can take up to 2 hours. Automatic maintenance can be enforced for minor version upgrade.</p>"},{"location":"amazonmq/#topology","title":"Topology","text":"<p>Amazon MQ - Active MQ supports the same topologies as the open-source version.</p> <p>In the Amazon MQ, the primary AWS resources are the Amazon MQ message broker and its configuration. The broker can be deployed in SINGLE_INSTANCE, ACTIVE_STANDBY_MULTI_AZ, or CLUSTER_MULTI_AZ. The configuration can be done via the AWS console, AWS CLI, Cloud Formation or CDK</p>"},{"location":"amazonmq/#active-standby","title":"Active Standby","text":"<p>Active / Standby topology uses a pair of brokers in different Availability Zones. One broker gets all the connection and traffic, the other is in standby, ready to take the traffic in case of failure of the active broker. The persistence is supported by a Storage Area Network.</p> <p></p> <p>Figure 3: Amazon MQ - Active/standby shared storage</p> <p>Amazon Elastic File System is the serverless file system used to persist messages. We can mount the EFS file systems on on-premises data center servers when connected to the Amazon VPC with AWS Direct Connect or AWS VPN.</p> <p>For an active/standby broker, Amazon MQ provides two ActiveMQ Web Console URLs, but only one URL is active at a time. Adding failover in broker url like:</p> <pre><code>failover:(ssl://b-9f..7ac-1.mq.eu-west-2.amazonaws.com:61617,ssl://b-9f...c-2.mq.eu-west-2.amazonaws.com:61617)\n</code></pre> <p>ensures that whenever server goes up, it will reconnect it immediately. See Active MQ documentation on failover</p> Network mapping <p>On AWS, each of those failover URL are in fact mapped to IP@ of a ENI. Each broker node has two ENIs connected to two different networks. The <code>b-9f...-1</code> is mapped to 10.42.1.29 for example on subnet 1, while <code>b-9f...-2</code> is 10.42.0.92 to subnet 0.</p>"},{"location":"amazonmq/#hybrid-cloud-with-aws","title":"Hybrid cloud with AWS","text":"<ul> <li> <p>During migration to the cloud, we need to support hybrid deployment where existing applications on-premises consume messages from queues or topics defined in Amazon MQ - Active MQ engine. The following diagram illustrates different possible integrations and the deployment of active/standby brokers in 2 availability zones.</p> <p></p> <p>Figure 4: Hybrid integration with Amazon MQ</p> <ul> <li>The on-premises applications or ETL jobs access the active broker, using public internet or private connection with Amazon Direct Connect, or site-to-site VPN.</li> <li>For the public access, the internet gateway routes the traffic to a network load balancer (layer 4 TCP routing), which is also HA (not represented in the figure), to reach the active Broker.</li> <li>The public internet traffic back from the Active MQ queue or topic to the consumer is via a NAT gateway. NAT gateways are defined in both public subnets for HA.</li> <li>When using private gateways, the VPC route tables includes routes to the CIDR of the on-premises subnets.</li> <li>Security group defines firewall like policies to authorize inbound and outbound traffic. The port for Active MQ needs to be open. Below is such declaration:</li> <li>EFS is used as a shared file system for messages persistence.</li> <li>The standby broker is linked to the active broker and ready to take the lead in case of active broker failure.</li> <li>For higher bandwidth and secured connection, Direct Connect should be used and then the communication will be via private gateway end point.</li> <li>Lambda function may be used to do light processing like data transformation, or data enrichment and then to call directly SaaS services. When more complex flow, like stateful flows, are needed, Amazon Step function can also be used (also serverless).</li> </ul> </li> </ul>"},{"location":"amazonmq/#mesh","title":"Mesh","text":"<p>Amazon MQ proposes a mesh network of single-instance brokers with non shated files as each broker uses EBS volume.</p> <p></p> <p>Figure 5: Amazon MQ broker mesh cluster deployment</p>"},{"location":"amazonmq/#security-considerations","title":"Security considerations","text":"<p>Active MQ is coming with its own way to define access control to Queue, Topics and Brokers. </p> <ul> <li>Access to AWS Console and Specific engine console to administrators.</li> <li>Encryption in transit via TLS.</li> <li>Encryption at rest using KMS: when creating the broker, we can select the KMS key to use to encrypt data.</li> <li>VPC support for brokers isolation and applications isolation.</li> <li>Security groups for firewall based rules.</li> <li>Queue/topic authentication and authorization using Configuration declarations.</li> <li>Integrated with CloudTrail for Amazon MQ API auditing.</li> <li>User accesses can be defined in an external LDAP, used for management Console access or for service accounts. Apps identifications are done inside the broker configuration.</li> </ul> <p>Amazon MQ uses IAM for creating, updating, and deleting operations on the message broker or configuration, and native ActiveMQ authentication for connection to brokers. The following figure illustrates those security contexts:</p> <p></p> <p>We define three users:</p> <ol> <li>An IAM administrator who manages security within an AWS account, specifically IAM users, roles and security policies. This administrator has full access to CloudWatch logs and CloudTrail for API usage auditing. The IAM policy uses action on \"mq:\" prefix, and possible resources are <code>broker</code> and <code>configuration</code>. </li> <li>An MQ service administrator, manages the MQ brokers and configuration via the AWS Console, AWS CLI or APIs. This administrator should be able to define brokers and configurations, networking access controls, security groups, and may be anything related to consumer and producer apps. He/she should have access to CloudWatch Logs and CloudTrail logs too. An administrator needs to signin to amazon api and get the permissions to act on the broker and the underlying EC2 instances.</li> <li>Developer defining queue, topics, and get broker URL and credentials. Developer users can be defined in external active directory or in broker configuration file.</li> </ol> <p>Amazon MQ management operations like creating, updating and deleting brokers require IAM credentials and are not integrated with LDAP.</p> <p>As the Amazon MQ control plane will do operations on other AWS services, there is a service-linked role (<code>AWSServiceRoleForAmazonMQ</code>) defined automatically when we define a broker, with the security policies (AmazonMQServiceRolePolicy) to get the brokers deployed. A service-linked role is a unique type of IAM role that is linked directly to Amazon MQ.</p> <p>Amazon MQ uses ActiveMQ's Simple Authentication Plugin to restrict reading and writing to destinations. See this product documentation for details.</p>"},{"location":"amazonmq/#ldap-integration","title":"LDAP integration","text":"<ul> <li> <p>The LDAP integration is done via ActiveMQ JAAS plugin.</p> <p></p> </li> <li> <p>A service account, defined in LDAP, is required to initiate a connection to an LDAP server. It sets up LDAP authentication for the brokers. Client connections are authenticated through this broker-LDAP connection.</p> </li> <li>The on-premises LDAP server needs a DNS name, and be opened on port 636.</li> <li>When creating broker, we can specify the LDAP login configuration with User Base distinguished name, search filter, role base DN, role base search filter. The user base supplied to the ActiveMQ broker must point to the node in the Directory Information Tree where users are stored in the LDAP server.</li> </ul> <p>As illustrated in figure above, we have to assess the different use cases:</p> <ol> <li>A user accessing the MQ console, the authentication to the broker console will go to the LDAP server</li> <li>Applications, producers or consumers, accessing the broker using the transport Connector URL, and authenticate via a service user defined in LDAP.</li> <li>An administrator users access Amazon MQ control plane via API, using AWS CLI, to create brokers, configurations. This user needs to be part of the <code>amazonmq-console-admins</code> group.</li> </ol> <p>For user authentication via LDAP we need to define the connection to LDAP in the broker configuration file, which also includes where to search the JMS topic and queue information in the DIT:</p> <pre><code>&lt;plugins&gt;\n    &lt;jaasAuthenticationPlugin configuration=\"LdapConfiguration\" /&gt; \n    &lt;authorizationPlugin&gt; \n     &lt;map&gt; \n       &lt;cachedLDAPAuthorizationMap\n            queueSearchBase=\"ou=Queue,ou=Destination,ou=ActiveMQ,dc=systems,dc=anycompany,dc=com\"\n            topicSearchBase=\"ou=Topic,ou=Destination,ou=ActiveMQ,dc=systems,dc=anycompany,dc=com\"\n            tempSearchBase=\"ou=Temp,ou=Destination,ou=ActiveMQ,dc=systems,dc=anycompany,dc=com\"\n            refreshInterval=\"300000\"\n            legacyGroupMapping=\"false\"\n        /&gt;\n         ...\n</code></pre> <ul> <li>Authorization is done on a per-destination basis (or wildcard, destination set) via the <code>cachedLdapAuthorizationMap</code> element, found in the broker\u2019s <code>activemq.xml</code>. See Amazon MQ product doc for xml examples and Active MQ doc with OpenLDAP example.</li> <li>In LDAP, we can define topics and queues in a Destination OU like: <code>dn: cn=carrides,ou=Queue,ou=Destination,ou=ActiveMQ,ou=systems,dc=anycompany,dc=com</code>, within those OU, either a wildcard or specific destination name can be provided (OU=ORDERS.$). Within each OU that represents a destination or a wildcard, we must create three security groups: admin, write, read, to include users or groups who have permission to perform the associated actions.</li> </ul> <pre><code>dn: cn=admin,cn=carrides,ou=Queue,ou=Destination,ou=ActiveMQ,ou=systems,dc=anycompany,dc=com \ncn: admin \ndescription: Admin privilege group, members are roles \nmember: cn=admin \nmember: cn=webapp \nobjectClass: groupOfNames \nobjectClass: top \n</code></pre> <p>Adding a user to the admin security group for a particular destination will enable the user to create and delete that queue or topic.</p>"},{"location":"amazonmq/#faqs","title":"FAQs","text":"What needs to be done to migrate to Artemis <p>As of today Amazon MQ, Active MQ supports only Classic deployment and API. Moving to Artemis, means deploying on your own EC2 instances. Most of the JMS code will work or with minimum refactoring for the connection factory. The project dependencies need to be changed, the ActiveMQ connection factory class is different in term of package names, and if you use Jakarta JMS then package needs to be changed in the JMS producer and consumer classes.</p> What are the CLI commands that can be run on Amazon MQ? <p>See this list. To run those we need a user or an IAM role with <code>mq:</code> actions allowed. </p> Support of JMX for broker and destination management? <p>Amazon MQ does not support JMX access, so queue needs to be created using code.</p> what are the critical metrics / log patterns that should be monitored in respect to MQ logs? <p>Amazon CloudWatch metrics has a specific Amazon MQ dashboard with CpuUtilization, CurrentConnectionCount, networking in/ou, producer and consumer counts. We can add our own metrics using the broker or queue specific ones. The following may be of interest for storage: (See this re:post):</p> <ul> <li>Store Percentage Usage</li> <li>Journal Files for Full Recovery: # of journal files that are replayed after a clean shutdown.</li> <li>Journal Files for Fast Recovery: same but for unclean shutdown. (too many pending messages in storage)</li> </ul> <p>When broker starts to have memory limit for a destination, then producer flow will be throttled, even blocked. (See this note)</p>"},{"location":"amazonmq/#good-sources-of-information","title":"Good sources of information","text":"<ul> <li>Amazon MQ - Active MQ product doc</li> <li>AWS Active MQ Workshop</li> <li>git amazon-mq-workshop</li> <li>Create broker AWS CLI command.</li> <li>Amazon MQ CLI.</li> <li>Using Amazon MQ as an event source for AWS Lambda</li> <li>Implementing enterprise integration patterns with AWS messaging services: point-to-point channels.</li> <li>CloudFormation template from the MQ Workshop.</li> <li>How do I troubleshoot Amazon MQ broker connection issues?.</li> </ul>"},{"location":"amqp-activemq/","title":"AMQP","text":"<p>The activeMQ folder includes AMQP clients based on the Quarkus guides for AMQP. In pure dev mode, quarkus starts AMQP broker automatically.</p> <pre><code># in one terminal\nmvn -f amqp-quickstart-producer quarkus:dev\n# in a second terminal\nmvn -f amqp-quickstart-processor quarkus:dev\n</code></pre> <p>Open http://localhost:8080/quotes.html in your browser and request some quotes by clicking the button.</p> <p>With docker compose it uses ActiveMQ image.</p> <pre><code>mvn -f amqp-quickstart-producer clean package\nmvn -f amqp-quickstart-processor clean package\n</code></pre>"},{"location":"rabbitmq/","title":"Rabbit MQ","text":"<p>RabbitMQ is an OOS message broker with highly flexible routing capability.</p> <p>Some Rabbit MQ important concepts:</p> <ul> <li>Exchanges: take a message and route it into zero or more queues or other exchanges. The supported types are <code>Direct, Topic, Fanout (like a pub/sub), Headers</code>. Exchanges are routing rules.</li> <li>Bindings: rules used by exchanges to route messages to queues.</li> <li>Queues: store messages. Single instance broker uses EBS as local storage.</li> </ul> <p></p> <p>Some key characteristics:</p> <ul> <li>RabbitMQ sends acknowledgements to publishers on message receipt</li> <li>Consumers maintain persistent TCP connections with RabbitMQ and declare which queue(s) they consume. Messages are pushed to consumers.</li> <li>Consumers send acknowledgements of success/failure.</li> <li>Messages are removed from queues once consumed successfully.</li> <li>We can have competing consumers to scale the message processing</li> <li>RabbitMQ offers \"at most once delivery\" and \"at least once delivery\"</li> <li>Consumer can configure a prefetch limit to avoid falling behind when producers send message faster than can be consumed.</li> <li><code>Direct</code> exchanges route messages to queues/exchanges that have a Binding Key that exactly matches the routing key.</li> <li><code>Header</code> exchanges route messages according to those header values.</li> <li><code>Consistent Hashing</code> exchange that hashes either the routing key or a message header and routes to one queue only. This is used for order guarantees with scaled out consumers.</li> <li>The publisher can set the lifetime of the message and also the queue can have a message TTL.</li> <li>Exchanges and queues can be dynamically created and given auto delete characteristics. It is used for ephermal reply queues.</li> <li>RabbitMQ offers a set of plugins (consisten hashing exchange, STOMP, MQTT, web hooks, SMTP,...). For example the Management Plug-In that provides an HTTP server, with web UI and REST API.</li> <li> <p>RabbitMQ offers Single Active Consumer (SAC) which prevents more than one consumer actively consuming a queue at the same time. So to cover scaling out and message ordering use the Consistent Hash Exchange and manually implement the consumer group logic of Kafka ourselves by using Single Active Consumer and custom hand-rolled logic.</p> </li> <li> <p>See sample example in labs/messaging/rabbitmq</p> </li> <li>Summary from this article</li> </ul>"},{"location":"rabbitmq/#hybrid-cloud-with-rabbit-mq","title":"Hybrid Cloud with Rabbit MQ","text":"<p>The classical needs are to move existing messaging solution running on-premises to cloud. Move the management to AWS will bring the operational responsability to AWS (product upgrage, infrastructure management). </p> <ul> <li>Easy to describe the cluster configuration using code, with cloud formation for ex. From single instance or three-node, clustered broker.</li> <li>Encryption in transit over TLS and at rest with KMS keys.</li> <li>Supports ActiveMQ, Rabbit MQ, AMQP.</li> </ul> <p>When moving from a single node to clustered there are some items to consider:</p> <ul> <li>Amazon MQ mirror queue configuration on nodes.</li> <li>Node replacement is done automatically. Consumers are disconnected and must reconnect. Queues automatically synchronize when a new mirror joins.</li> <li>Queue synchronization is a blocking operation per queue</li> <li>Keep queue size small by consuming messages quickly to minimize synchronization time.</li> <li>Public access to the broker: facade with a NLB. In private VPC, use VPC endpoint to NLB. NLBs are per AZ, so in a 3 node cluster, we have 3 NLBs.</li> <li> <p>For migration there are two plugins that can be used: </p> <ul> <li>Federation plugin: downstream brokers initiate one-way, point-to-point connections to upstream brokers. Federated queues move messages from upstream queues to perform load balancing across downstream consumers. Fedreated exchanges copy messages from upstream exchanges to downstream ones, using connected topologies such as pairing, graphs, and fan-out.</li> <li>Shovel plugin: the approach is to consume from source broker's queue and move message to a queue or exchange on a destination broker. It supports flexible topology but does not detect loop.</li> </ul> </li> </ul> <p>Below is a figure to illustrate an architecture to migrate from on-premises broker (without public access) to AWS managed cluster using Shovel plugin.</p> <p></p> <p>The same approach with a public access at the source level:</p> <p></p> <p>Here is a set of AWS CLI commands to get information of the brokers:</p> <pre><code># change the name of the broker if needed\nbrokerId=$(aws mq list-brokers | jq '.BrokerSummaries[] | select(.BrokerName==\"demo-jb\") | {id:.BrokerId}' | grep \"id\" | cut -d '\"' -f4)\nurl=$(aws mq describe-broker --broker-id=$brokerId | jq '.BrokerInstances[].Endpoints[0]' | xargs -n 2 | awk '{ print \"failover:(\"$1\",\"$2\")\" }')\nuserPassword=$(aws ssm get-parameter --name \"MQBrokerUserPassword\" |&amp; grep \"Value\\|ParameterNotFound\")\n</code></pre>"},{"location":"req-reply-jms/","title":"Active MQ Artemis - JMS based request-replyTo demonstration","text":"<p>The application is the same as the classic implementation as described in this note, except, it uses the Artemis release with Jakarta JMS 3.0 API. This is not supported in Amazon MQ as of now.</p>"},{"location":"req-reply-jms/#running-locally","title":"Running Locally","text":"<p>While in development mode, under the artemis/request-replyto folder:</p> <ol> <li>Start Active MQ: <code>docker compose up -d</code></li> <li> <p>Start each application with <code>quarkus dev</code></p> <pre><code>cd jms-orchestrator\nquarkus dev\ncd jms-participant\nquarkus dev\n</code></pre> </li> <li> <p>Orchestrator Application URL: http://localhost:8081/ and swagger-ui</p> </li> <li>See ActiveMQ console: http://localhost:8161/, admin/adminpassw0rd</li> </ol>"},{"location":"req-reply-jms/#demonstration-scripts","title":"Demonstration scripts","text":"<ol> <li> <p>Post new order using the exposed API by going under <code>request-replyto/e2e</code> folder:</p> <pre><code>cd request-replyto/e2e\n./postOrder.sh\n</code></pre> </li> <li> <p>In the Orchestrator trace we should see  the following message, from the Resource API, then getting the response from the participant of the order process as the status for order changed from <code>pending</code> to <code>assigned</code></p> <pre><code>processing new order: { \"sku\": P05,\"price\": 100.0,\"quantity\": 2,\"status\": pending }\n17:34:32 INFO  [or.ac.or.in.re.OrderRepositoryMem] (executor-thread-1) Save in repository 7cfef528\n17:34:32 INFO  [or.ac.or.in.ms.OrderMessageProcessing] (Thread-2 (ActiveMQ-client-global-threads)) Received message: b1c0191f-4533-44f9-b56d-18ec67baa1fc,7cfef528,P05,100.0,2,assigned\n</code></pre> </li> <li> <p>On the participant side the log looks like:</p> <pre><code>Received message: b1c0191f-4533-44f9-b56d-18ec67baa1fc,7cfef528,P05,100.0,2,pending\n17:34:32 INFO  [or.ac.pa.in.ms.OrderMessageConsumer] (Thread-1 (ActiveMQ-client-global-threads)) Reponse sent to replyTo queue {\"messageID\":\"b1c0191f-4533-44f9-b56d-18ec67baa1fc\",\"orderID\":\"7cfef528\",\"sku\":\"P05\",\"price\":100.0,\"quantity\":2,\"status\":\"assigned\"}\n</code></pre> </li> <li> <p>We can see the state of the queues in the ActiveMQ Console</p> <p></p> </li> </ol>"},{"location":"req-reply-jms/#code-explanation","title":"Code Explanation","text":"<p>The code is under jms-orchestrator and jms-participant, to implement a request-response over queue using JMS.</p> <p>The Orchestrator is a classical microservice with the order entity as resource. The interesting part is the <code>OrderMessageProcessing</code> class. It is JMS implementation code, using one connection to the broker and two JMS sessions, one for the producer and one for the consumer.</p> <pre><code>connectionFactory = new ActiveMQConnectionFactory(connectionURLs);\nconnection = connectionFactory.createConnection(user, password);\nconnection.setClientID(\"p-\" + System.currentTimeMillis());\n\ninitProducer();\ninitConsumer();\nconnection.start();    \n</code></pre> <p>As the class is MessageListener, the JMSconsumer thread is associate to it, and the <code>OrderMessageProcessing</code> class processes the replyTo queue messages in the onMessage method.</p> <pre><code> public void onMessage(Message msg) {\n       TextMessage rawMsg = (TextMessage) msg;\n       OrderMessage om;\n        try {\n            om = mapper.readValue(rawMsg.getText(),OrderMessage.class);\n            logger.info(\"Received message: \" + om.toString());\n            Order o = OrderMessage.toOrder(om);\n            service.processParticipantResponse(o);\n            msg.acknowledge();\n        }\n        catch (JsonProcessingException | JMSException e) {\n            e.printStackTrace();\n        }\n    }\n</code></pre> <p>Two implementation practices: having a different data model for the message than the business entity, and use the service class to implement the business logic.</p> <p>On the consumer side, the class is also a MessageListener, but also a producer to the different queue. Each consumer performs acknowledgement by code. Acknowledging a consumed message automatically acknowledges the receipt of all messages that have been consumed by the current session.</p>"},{"location":"sns/","title":"Amazon SNS - Simple Notification Service","text":"Amazon SNS elevator pitch <p>Amazon SNS is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging.</p> <p>Producer sends message to one SNS Topic. SNS pushes data to subscribers. </p> <p>SNS supports up to 12,500,000 subscriptions per topic, 100,000 topic limits per account. For subscription filter policies, the default per topic limit, per AWS account, is 200. The filter policies' limit per AWS account is 10,000.</p> <p>Priced by number of messages published, the number of notifications, and number of API calls. </p>"},{"location":"sns/#producing-messages","title":"Producing messages","text":"<p>The producers can publish to topic via SDK or can use different protocols like: HTTP / HTTPS (with delivery retries \u2013 how many times), SMTP,  SMS, ... </p> <p>Many AWS services can send data directly to SNS for notifications: CloudWatch (for alarms), AWS budget, Lambda, Auto Scaling Groups notifications, Amazon S3 (on bucket events), DynamoDB, CloudFormation, AWS Data Migration Service, RDS Event...</p> <p>SNS messages may be archived via adding Amazon Data Firehose as a subscriber, then the destination becomes S3, Redshift tables, OpenSearch... Message replay is then custom.</p> <p>SNS FIFO topics support an in-place, no-code, message archive that lets topic owners store (or archive) messages published to a topic for up to 365 days.</p>"},{"location":"sns/#consuming-messages","title":"Consuming messages","text":"<p>Each subscriber to the topic will get all the messages. As data is not persisted, we may lose messages not processed within a specific time window.</p> <p>SNS can filter message by using a JSON policy attached to the SNS topic's subscription. By default all messages go to subscribers. SNS compares the message attributes or the message body to the properties in the filter policy for each of the topic's subscriptions, in case of match the message is sent to the subscriber. See some filter policy examples.</p> <p>The subscribers can be a SQS queue, a HTTPs endpoint, a Lambda function, Kinesis Firehose, EventBridge, Emails... But not Kinesis Data Streams.</p> <p>SNS in EDA can be used by adding subscription to event-handling pipelines\u2014powered by AWS Event Fork Pipelines. See samples here.</p> Eventual consistency <p>Change to a subscription filter policy require up to 15 minutes to fully take effect. This duration can't be reduced.</p>"},{"location":"sns/#security","title":"Security","text":"<p>For security needs, it supports HTTPS, and encryption at rest with KMS keys. For access control, IAM policies can be defined for the SNS API (looks like S3 policies). Same as SQS, used for cross account access and with other services.</p>"},{"location":"sns/#combining-with-sqs-fan-out-pattern","title":"Combining with SQS - Fan Out pattern","text":"<p>SNS can be combined with SQS: Producers push once in SNS, and messages are receive in all SQS queues, subscribers to the topic It is fully decoupled without any data loss. SQS allows for data persistence, delayed processing and retries.</p> <ul> <li>An application pushes once in a SNS Service, and the SQS queues are subscribers to the topic and then get the messages. Fan Out.</li> <li>Fully decoupled with no data loss as SQS will always listen to SNS topic.</li> <li>SQS adds data persistence, delayed processing and retries of work.</li> <li>Increase the number of subscriber over time.</li> <li>Using SNS FIFO and SQS FIFO it will keep ordering.</li> <li>Can use Message Filtering using a JSON policy .</li> <li>Can be used for cross-region delivery, with a SQS queue in another region.</li> </ul>"},{"location":"sqs/","title":"SQS: Standard queue","text":"Document information <p>Created Oct 2022 - Updated 02/2024</p> Amazon SQS elevator pitch <p>Amazon SQS is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. Using Amazon SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.</p>"},{"location":"sqs/#basic-concepts","title":"Basic concepts","text":"<ul> <li>Oldest queueing service on AWS. Full managed service.</li> <li>Unlimited throughput and unlimited number of message in queue.</li> <li>The default retention is 4 days up to 14 days. low latency &lt; 10ms. </li> <li>Max message size is 256KB. Can be configured at the queue level. If messages are &gt; 256k, use the claim-check pattern using S3 bucket.</li> <li>By default duplicate message is possible (at least once delivery) and out of order too (best effort).</li> <li>Consumer deletes the message.</li> <li>Supports automatic scaling.</li> <li>Max delay is 15 minutes</li> <li>Max queue size is 120k for standard and 20k for FIFO.</li> <li>SQS can be used as a write buffer for DynamoDB using a Lambda or other app to do the write.</li> <li>See serverlessland patterns</li> </ul> <p>Specific SDK to integrate to <code>SendMessage</code>, <code>GetMessage</code>...</p> <p>Consumers receive, process and then delete the messages. Parallelism is possible. The consumers can be in an auto scaling group (ASG) and with CloudWatch, it is possible to monitor the queue size / # of instances and in the CloudWatch alarm action, triggers EC2 scaling. </p> <p></p> <p>Figure 1: Auto scaling with  CloudWatch</p> <p>Message has metadata out of the box. After a message is polled by a consumer, it becomes invisible to other consumers.</p> <p></p> <p>Figure 2: Messsage over SQS Metadata</p>"},{"location":"sqs/#fan-out-pattern","title":"Fan-out pattern","text":"<p>When we need to send the same message to more than one SQS consumer, we need to combine SNS and SQS: message is sent to the SNS topic and then \"fan-out\" to multiple SQS queues. It's fully decoupled, no data loss, and we have the ability to add more SQS queues (more applications) over time.</p> <p>This is the pattern to use if consumer may be down for a long period: solution based on SNS only, will make some messages not processed and lost.</p>"},{"location":"sqs/#visibility-timeout","title":"Visibility timeout","text":"<p>By default, the \u201cmessage visibility timeout\u201d is 30 seconds, which means the message has 30 seconds to be processed (Amazon SQS prevents other consumers from receiving and processing the message). If a consumer fails to process a message within the Visibility Timeout, the message goes back to the queue.</p> <p></p> <p>Figure 3: SQS message visibility</p> <p>After the message visibility timeout is over, the message is \u201cvisible\u201d in SQS, therefore the message may be processed twice. But a consumer could call the <code>ChangeMessageVisibility</code> API to get more time to process. When the visibility timeout is high (hours), and the consumer crashes then the re-processing of all the messages will take time. If it is set too low (seconds), we may get duplicates.</p> <p>To reduce the number of API call to request message (improve latency and app performance), consumer can use the <code>long polling</code> API and wait for message arrival. </p> <p>When processing SQS messages in batches, if one of those messages cannot be processed, the consumer needs to handle the failure and returns a partial batch response, so only failed messages will be re-processed.  See best practices for batch handling.</p> <p>If you do not handle the error and just raise an exception on Lambda side, the whole batch will be reprocessed, including successfully processed messages.</p>"},{"location":"sqs/#sqs-dead-letter-queue","title":"SQS Dead Letter Queue","text":"<p>We can set a threshold for how many times a message can go back to the queue.</p> <p>After the <code>MaximumReceives</code> threshold is exceeded, the message goes into a dead letter queue (DLQ) (which has a limit of 14 days to process).</p> <p></p> <p>Figure 4: Dead letter queue configuration</p> <p>Delay queues let us postpone the delivery of new messages to a queue for a number of seconds. If we create a delay queue, any messages that we send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.</p> <p>See redrive back to source to get the message back from the DLQ to the source queue (once the consumer application code is modified for example).</p>"},{"location":"sqs/#sqs-fifo-queue","title":"SQS FIFO Queue","text":"<p>Queue can be set as FIFO to guaranty the order: limited to throughput at 300 msg/s without batching or 3000 msg/s with batching. It can also support exactly once delivery. While configuring the FIFO queue a parameter (ContentBasedDeduplication) can be set to remove duplicate by looking at the content: Every message must have a unique <code>MessageDeduplicationId</code>.</p> <p>The name of the queue has to end with <code>.fifo</code>.</p> <p>If we don't use a Group ID, messages are consumed in the order they are sent, with only one consumer. But using Group ID, we can have as many consumers as there is groups. It looks like partition key in kinesis data streams. Each consumer will get ordered records.</p>"},{"location":"sqs/#security","title":"Security","text":"<p>For security, there is encryption in flight via HTTPS, and at rest with KMS keys. SQS API access control via IAM policies, and SQS Access Policies for cross-account access and for other AWS services to access SQS.</p> <p>It comes with monitoring.</p>"},{"location":"sqs/#sample-code","title":"Sample Code","text":"<ul> <li>Python boto3 SQS</li> <li>See consumerSQS and produceSQS in Python study / aws folder</li> </ul>"},{"location":"labs/activemq-cdk/","title":"Deploy ActiveMQ with AWS CDK","text":"<p>Info</p> <p>Created 09/2023 - Updated 11/27/23</p> <p>Pre-requisites: CDK, AWS cli installed on developer host or Cloud9 environment.</p>"},{"location":"labs/activemq-cdk/#common-stack","title":"Common Stack","text":"<p>The first stack is used to define a VPC with 2 AZs, 2 public subnets, 2 private subnets, 1 NAT gateway (for cost reason, in real production deployment we should have two NATs), route tables with default routes, and Network Load Balancer. The deployment looks like in the following diagram:</p> <p></p> <p>To create the stack go to the <code>IaC-common</code> folder and use <code>cdk</code> CLI.</p> <pre><code>cd IaC-common\n# under IaC-common folder\ncdk synth\ncdk deploy --all\n</code></pre> <p>Here are the resource created:</p> <ul> <li>One internet gateway, 2 NAT gateways, one in each in public subnet. Each NAT gateway has a Elastic Network Interface with public and private IP addresses.</li> <li>2 route tables, one in each public subnet and one in each private subnet</li> <li>2 route tables for the private subnet, that have egress route to NAT</li> <li>Security groups</li> <li>IAM role for the lambda function to assume, so it can update security groups in VPC</li> <li>Lambda function for removing all inbound/outbound rules from the VPC default security group</li> <li>Cloud 9 environment</li> </ul> <p>For a production deployment, using Amazon MQ clustering, we will use a 3 AZs deployment as in the figure below:</p> <p></p>"},{"location":"labs/activemq-cdk/#active-mq-activestandby","title":"Active MQ Active/Standby","text":"<p>To deploy an Amazon MQ - Active MQ with active and standby deployment use the stack under amazonMQ/activeMQ/IaC/active-standby which adds the following elements to current physical architecture:</p> <p></p> <p>The CDK gets the reference of the DemoVPC and will deploy the broker inside the private subnets.</p> <p>Important, there are no official CDK hand-written (L2) constructs for Amazon MQ, so the APIs used are amazonmq.CfnBroker and ConfigurationIdProperty.</p> <p>Configuration can only referenced, so we need to create them manualy or using aws cli.</p> <p>Before running <code>cdk deploy</code> verify the current version of Active MQ.</p>"},{"location":"labs/activemq-ldap-sec/","title":"Active MQ with LDAP and security lab","text":""},{"location":"labs/activemq-ldap-sec/#deploying-an-openldap-on-ec2","title":"Deploying an OpenLDAP on EC2","text":"<ol> <li>Create a EC2 instance with Ubuntu, t3.micro, and create a security group with port 22 and HTTP 80, from my IP address.</li> <li> <p>Install Apache HTTPD server</p> <p><pre><code>sudo apt-get update\nsudo apt-get install apache2\n</code></pre> 1. Install OpenLDAP with adminpassw0rd</p> <pre><code>sudo apt-get install slapd ldap-utils\n</code></pre> </li> <li> <p>Start the configuration of LDAP</p> <pre><code>sudo dpkg-reconfigure slapd\n# Omit OpenLDAP server configuration? No\n# acme.com as DNS\n# ACME for org\n# Remove the database when slapd is purged? No\n# Move old database?  Yes\n</code></pre> </li> <li> <p>Install Web App for OpenLDAP</p> <pre><code>sudo apt-get install phpldapadmin\n# update the config\nsudo vi /etc/phpldapadmin/config.php\n# Modify the following:\n# $servers-&gt;setValue('server','base',array('dc=acme,dc=com'));\n# $servers-&gt;setValue('login','bind_id','cn=admin,dc=acme,dc=com');\n</code></pre> </li> <li> <p>Go to the admin console: http://ec2-35-86-175-148.us-west-2.compute.amazonaws.com/phpldapadmin/</p> </li> </ol>"},{"location":"labs/activemq-ldap-sec/#configuring-active-mq-jetty-to-access-ldap","title":"Configuring Active MQ jetty to access LDAP","text":""},{"location":"labs/activemq-ldap-sec/#configure-brokers-to-access-ldap","title":"Configure brokers to access LDAP","text":""},{"location":"labs/activemq-ldap-sec/#iam-work","title":"IAM work","text":""},{"location":"labs/activemq-monitoring/","title":"ActiveMQ monitoring","text":""},{"location":"labs/activemq-monitoring/#local-jmx-test","title":"Local JMX test","text":"<p>Broker configuration define the connector and enable jmx:</p> <pre><code> &lt;broker xmlns=\"http://activemq.apache.org/schema/core\" brokerName=\"localhost\" dataDirectory=\"${activemq.data}\" useJmx=\"true\"&gt;\n        ...\n        &lt;managementContext&gt;\n             &lt;managementContext createConnector=\"true\" connectorPort=\"1099\" connectorHost=\"localhost\"/&gt;\n        &lt;/managementContext&gt;\n</code></pre> <p>The docker compose files under  has the following java options to configure remote jmx access:</p> <pre><code>    ACTIVEMQ_OPTS: \"-Dcom.sun.management.jmxremote.port=38120 -Djava.rmi.server.hostname=$HOSTNAME -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false\"\n</code></pre> <p>and the exported ports are: 1099 and 38120.</p> <p>Once started the logs of the activeMQ java app lists the jmx URL for remote access: <code>activemq  |  INFO | JMX consoles can connect to service:jmx:rmi:///jndi/rmi://localhost:1099/jmxrmi</code></p> <p>Start <code>jconsole</code> from the Java JDK and then define the remote connection with the previous URL:</p> <p></p> <p>which after some time, to get the MBeans discovered should lead to the ActiveMQ MBeans screen. From there we can navigate to the queue created by one of the producer app:</p> <p></p> <p>Within the operations we can add new queue:</p> <p></p> <p>The new <code>carrides.dlq</code> is added.</p> <p>From there it will be possible to move message from one queue (the DLQ one) to one of the processing queue using the move message operation at the queue level:</p> <p></p>"},{"location":"labs/activemq-monitoring/#amazon-mq-activemq-engine","title":"Amazon MQ - ActiveMQ engine","text":"<p>JMX is not yet exposed on Amazon MQ - ActiveMQ broker engine: the useJmx attribute on the broker configuration is unsupported. For queue and topic metrics Amazon CloudWatch should give developers enough data points.</p> <p>Jolokia is not an option as it uses the broker JMX Mbeans to support management operations exposed as REST/JSON API.</p> <p>The ActiveMQ console is exposed to see queue and topic content.</p> <p>In case it is not suffisant we can develop a simple WebApp that exposes some basic operation like creating a queue or topic and move message from a queue to another one, like the JMX move messages. The REST API could includes the following constructs:</p> Path Operation Description Example Parameters /queues/ POST Create a new queue { \"name\" : \"carrides.dlq\" } a Json doc with properties for the queue /queues/ GET Get the list of queues a Json doc with queue descriptions /queues/ DELETE Delete the specified queue { \"name\" : \"carrides.dlq\" } a Json doc with queue name /queues/{queue_name}/moveMessageTo PUT Move a message from the queue as parameter to a destination, given the JMS message id { \"destinationName\" : \"carrides\", \"messageId\": \"...\"  } A Json doc with destination and messageId /queues/{queue_name}/moveMatchingMessageTo PUT Move messages matching a filter from the queue as parameter to a destination { \"destinationName\" : \"carrides\", \"selector\": \" a sql filter\" } A Json doc with destination and filter selector <p>This administration application can be deployed as a serverless on Amazon ECS Fargate, within the same VPC as the Amazon MQ brokers. The implementations of the mapper between REST api and Active MQ can use JMS, AMQ API and Amazon SDK.</p> <p></p> <p>See code sample in the amq-mgr folder with explanation on how to run it locally, and deploy it to AWS VPC.</p>"},{"location":"labs/aq-aws-console-lab/","title":"Amazon MQ - Active MQ creation via the AWS console","text":"<p>This note is a simple summary of the steps to deploy Amazon MQ brokers using AWS console, for Active MQ and Rabbit MQ engines.</p>"},{"location":"labs/aq-aws-console-lab/#active-mq","title":"Active MQ","text":"<p>The AWS console wizard is easy to get the brokers set up, and the most important elements of the configuration, is to select the engine, the type of deployment, the type of EC2 instance, the product version, VPC, user credential, type of maintenance, logging level.</p> <p>For demonstration purpose, we can use a single-instance broker, on <code>mq.t3.micro</code> instance type. EBS volumes are for lower latency and higher throughput and cannot be used for clustered brokers in HA deployment. It will scale to 500 msg/s per broker. EBS can be replicated within the same AZ. For HA, EFS is used.</p> <p>For production, we need to setup active/standby with shared persistence using Amazon EFS, or for more complex topology, use a broker Mesh.</p> <p>Here are some important configurations using AWS Console: </p> <ol> <li> <p>Select the deployment mode (see topology section) for diagrams. The blueprint offers the different broker mesh topologies.</p> <p></p> </li> <li> <p>Select a cluster name and the EC2 instance type:</p> <p></p> </li> <li> <p>In <code>Advanced</code> settings, select product version, VPC, user to authenticate apps and console... Broker configurations are separated elements, so can be reused between cluster.</p> <p></p> <p>To ensure that the broker is accessible within our VPC, we must enable the <code>enableDnsHostnames</code> and <code>enableDnsSupport</code> VPC attributes.</p> <p></p> <p>When developing solution, and deploying with public access, add our host IP address to the security group on port 8162 (use https://whatismyipaddress.com/)</p> </li> <li> <p>To allow Amazon MQ to publish logs to CloudWatch Logs, we must add a permission to our Amazon user and also configure a resource-based policy for Amazon MQ before creating or restarting the broker.</p> </li> <li> <p>Once created the <code>Connection section</code> includes the different URLs. </p> <p></p> <p>To access the console we need to update the inbound rules of the VPC default security group to add port 8162, and 61617.</p> </li> <li> <p>Access to the Active MQ Console with the admin user.</p> </li> <li> <p>Finally to test a producer and consumer apps, we can use a docker compose file that will start both apps but connect to the remote broker:</p> <pre><code>cd amazonMQ/activeMQ/ow-pt-to-pt-jms\nexport AMQ_URL=ssl://b-c05ed3bc-58e7-4309-bb9c-9c3a581228ee-1.mq.us-west-2.amazonaws.com:61617\ndocker compose -f aws-amq-docker-compose.yml up\n</code></pre> <p>We should get a Connec to broker suceed messages in the producer and the consumer apps. The queue should have been created by the code.</p> <p></p> <p>Sending some data with <code>./e2e/startSimulation.sh</code>. The execution trace demonstrates messages published and consumed.</p> </li> </ol> Broker Configuration <p>By default a configuration is created. An example of such configuration is under the amazonMQ/activeMQ/ow-pt-to-pt-jms/config folder.</p>"},{"location":"labs/classic-req-reply-jms/","title":"JMS based request-replyTo demonstration - ActiveMQ Classic","text":"<p>This is a simple example of two applications communicating asynchronously over two queues:</p> <p></p> <p>This code uses client acknowledgement, and replyTo queue. It does not use CDI for bean injection but code base instantiation of the ConnectionFactory. It also supports reconnect in case of connection to broker failure.</p> <p>The API used for JMS is 2.0 as ActiveMQ is the classic, version 5.13.</p>"},{"location":"labs/classic-req-reply-jms/#requirements","title":"Requirements","text":"<ul> <li>Expose GET, POST, PUT <code>/orders</code> api</li> <li>Mockup a repository in memory</li> <li>On POST or PUT operations, order messages are sent to another service (the participant) to act on them via a <code>orders</code> queue, and get the response to <code>orders-reply</code> queue.</li> <li>Support once and only once semantic</li> <li>Expose a POST /orders/simulation to run n order creation with random data, to support a failover demonstration.</li> </ul>"},{"location":"labs/classic-req-reply-jms/#running-locally","title":"Running Locally","text":"<p>While in development mode, under the <code>activeMQ/classic/request-replyto</code> folder:</p> <ol> <li>Start Active MQ: <code>docker compose up -d</code></li> <li> <p>Start each application with <code>quarkus dev</code></p> <pre><code>cd jms-orchestrator\nquarkus dev\ncd jms-participant\nquarkus dev\n</code></pre> </li> <li> <p>Orchestrator Application URL: http://localhost:8081/ and swagger-ui</p> </li> <li>See ActiveMQ console: http://localhost:8161/, admin/adminpassw0rd</li> </ol>"},{"location":"labs/classic-req-reply-jms/#demonstration-scripts","title":"Demonstration scripts","text":"<ol> <li> <p>Post new order using the exposed API by going under <code>request-replyto/e2e</code> folder:</p> <pre><code>cd request-replyto/e2e\n./postOrder.sh\n</code></pre> </li> <li> <p>In the Orchestrator trace we should see  the following message, from the Resource API, then getting the response from the participant of the order process as the status for order changed from <code>pending</code> to <code>assigned</code></p> <pre><code>processing new order: { \"sku\": P05,\"price\": 100.0,\"quantity\": 2,\"status\": pending }\n17:34:32 INFO  [or.ac.or.in.re.OrderRepositoryMem] (executor-thread-1) Save in repository 7cfef528\n17:34:32 INFO  [or.ac.or.in.ms.OrderMessageProcessing] (Thread-2 (ActiveMQ-client-global-threads)) Received message: b1c0191f-4533-44f9-b56d-18ec67baa1fc,7cfef528,P05,100.0,2,assigned\n</code></pre> </li> <li> <p>On the participant side the log looks like:</p> <pre><code>Received message: b1c0191f-4533-44f9-b56d-18ec67baa1fc,7cfef528,P05,100.0,2,pending\n17:34:32 INFO  [or.ac.pa.in.ms.OrderMessageConsumer] (Thread-1 (ActiveMQ-client-global-threads)) Reponse sent to replyTo queue {\"messageID\":\"b1c0191f-4533-44f9-b56d-18ec67baa1fc\",\"orderID\":\"7cfef528\",\"sku\":\"P05\",\"price\":100.0,\"quantity\":2,\"status\":\"assigned\"}\n</code></pre> </li> <li> <p>We can see the state of the queues in the ActiveMQ Console</p> <p></p> </li> <li> <p>To demonstrate connection failure and reconnect, start a long runnning simulation, stop the broker and relaunch it. The messages should continue to flow between the stop apps. Be sure to have build the producer and consumer images. Here are the commands:</p> <pre><code># build OCI image for participant and orchestrator app\nrequest-replyto $ ./buildAll.sh\n# We should have 2 new OCI images. \nrequest-replyto $ docker images\n\nrequest-replyto $ docker compose -f e2e-docker-compose.yml up -d\nrequest-replyto $ docker ps\n# 3cd51215160f   apache/activemq-classic:latest  0.0.0.0:5672-&gt;5672/tcp, 8080/tcp, 0.0.0.0:8161-&gt;8161/tcp, 0.0.0.0:61616-&gt;61616/tcp, 8443/tcp   active\n# 2ce293bc26bd   apache/activemq-classic:latest  0.0.0.0:5682-&gt;5672/tcp, 0.0.0.0:8171-&gt;8161/tcp, 0.0.0.0:61626-&gt;61616/tcp standby\n...\n</code></pre> <p>Start some messages under <code>classic/request-replyto/e2e</code></p> <pre><code>e2e $ ./startNorders.sh 50\n</code></pre> <p>Once some messages are exchanged stop the active broker.</p> </li> </ol>"},{"location":"labs/classic-req-reply-jms/#code-explanation","title":"Code Explanation","text":"<p>The code is under jms-orchestrator and jms-participant, to implement a request-response over queue using JMS.</p> <p>The Orchestrator is a classical microservice with the order entity as resource. The interesting part is the <code>OrderMessageProcessing</code> class. It is JMS implementation code, using one connection to the broker and two JMS sessions, one for the producer and one for the consumer.</p> <p>The API is jms 2.0 based on <code>javax.jms</code> API, and the ActiveMQ is the client app: here is the important maven declarations</p> <pre><code>    &lt;!-- https://mvnrepository.com/artifact/javax.jms/javax.jms-api --&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;javax.jms&lt;/groupId&gt;\n        &lt;artifactId&gt;javax.jms-api&lt;/artifactId&gt;\n        &lt;version&gt;2.0.1&lt;/version&gt;\n    &lt;/dependency&gt;\n\n    &lt;!-- https://mvnrepository.com/artifact/org.apache.activemq/activemq-client --&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt;\n        &lt;artifactId&gt;activemq-client&lt;/artifactId&gt;\n        &lt;version&gt;5.18.2&lt;/version&gt;\n    &lt;/dependency&gt;\n</code></pre> <p>The classical JMS implementation uses the ConnectionFactory, creates a unique connection to the broker, and then one session to send message and one to receive message from the different queue:</p> <pre><code>connectionFactory = new ActiveMQConnectionFactory(connectionURLs);\nconnection = connectionFactory.createConnection(user, password);\nconnection.setClientID(\"p-\" + System.currentTimeMillis());\n\ninitProducer();\ninitConsumer();\nconnection.start();    \n</code></pre> <p>As the class is MessageListener, the JMSconsumer thread is associated to it, and the <code>OrderMessageProcessor</code> class processes the replyTo queue messages in the onMessage method.</p> <pre><code> public void onMessage(Message msg) {\n    //...\n        OrderMessage oe = OrderMessage.fromOrder(order);\n        String orderJson= mapper.writeValueAsString(oe);\n        TextMessage msg =  producerSession.createTextMessage(orderJson);\n        msg.setJMSCorrelationID(UUID.randomUUID().toString().substring(0,8));\n        producer.send( msg);\n    //...\n</code></pre> <p>We should follow some implementation best practices:</p> <ul> <li>having a different data model for the message payload than the business entity persisted (Order and OrderMessage), </li> <li>use a service class to implement the business logic to manage the business entity.</li> <li>a different Resource class to support the RESTful APIs</li> </ul> <pre><code> orders\n  \u251c\u2500\u2500 domain\n  \u2502\u00a0\u00a0 \u251c\u2500\u2500 Order.java\n  \u2502\u00a0\u00a0 \u2514\u2500\u2500 OrderService.java\n  \u2514\u2500\u2500 infra\n      \u251c\u2500\u2500 api\n      \u2502\u00a0\u00a0 \u251c\u2500\u2500 OrderResource.java\n      \u2502\u00a0\u00a0 \u2514\u2500\u2500 SimulControl.java\n      \u251c\u2500\u2500 msg\n      \u2502\u00a0\u00a0 \u251c\u2500\u2500 OrderMessage.java\n      \u2502\u00a0\u00a0 \u2514\u2500\u2500 OrderMessageProcessor.java\n      \u2514\u2500\u2500 repo\n          \u251c\u2500\u2500 OrderRepository.java\n          \u2514\u2500\u2500 OrderRepositoryMem.java\n</code></pre> <p>The configuration of the application is based on the quarkus way to declare config:</p> <pre><code>main.queue.name=orders\nreplyTo.queue.name=ordersResp\nreconnect.delay.ins=5\n\n# Configures the Artemis properties.\nquarkus.artemis.url=tcp://localhost:61616\n</code></pre> <p>Which could be overwritten by environment variables as illustrated in the docker compose file:</p> <pre><code> environment:\n      QUARKUS_ARTEMIS_URL: failover:(tcp://active:61616,tcp://standby:61617)?randomize=false\n</code></pre> <p>On the consumer side, the class is also a MessageListener, and a producer to the replyTo queue.</p> <p>Each consumer performs acknowledgement by code. Acknowledging a consumed message automatically acknowledges the receipt of all messages that have been consumed by the current session.</p>"},{"location":"labs/classic-req-reply-jms/#some-activemq-configuration-explanation","title":"Some ActiveMQ configuration explanation","text":"<p>The config folder includes some activemq.xml for the active and standby broker and also the jetty.xml so the console is accessible from the host machine.</p> <ul> <li>Jetty configuration update include host to be 0.0.0.0</li> </ul> <pre><code>    &lt;bean id=\"jettyPort\" class=\"org.apache.activemq.web.WebConsolePort\" init-method=\"start\"&gt;\n             &lt;!-- the default port number for the web console --&gt;\n        &lt;property name=\"host\" value=\"0.0.0.0\"/&gt;\n        &lt;property name=\"port\" value=\"8161\"/&gt;\n    &lt;/bean&gt;\n</code></pre> <ul> <li>To be able to have active and standby to communicate and use shared storage, we need to add in active broker configuration the fact to use kahadb on a mounted end point and define a network connector to the partner:</li> </ul> <pre><code>    &lt;persistenceAdapter&gt;\n        &lt;kahaDB directory=\"/tmp/mq/kahadb\"/&gt;\n    &lt;/persistenceAdapter&gt;\n    &lt;networkConnectors&gt;\n        &lt;networkConnector uri=\"static:(tcp://standby:61626)\" /&gt;\n    &lt;/networkConnectors&gt;\n</code></pre>"},{"location":"labs/classic-req-reply-jms/#deploy-on-aws","title":"Deploy on AWS","text":"<ol> <li> <p>First build the docker images for each service using <code>buildAll.sh</code> command: Change the name of the image to adapt to your ECR repository.</p> <pre><code>cd jms-orchestrator\nbuildAll.sh\ndocker push toECR_repository\ncd jms-participant\nbuildAll.sh\ndocker push toECR_repository\n</code></pre> </li> <li> <p>If not already done, use CDK to deploy VPC, Brokers, and Cloud9.</p> </li> <li> <p>Use CDK to deploy the two apps on ECS Fargate.</p> <pre><code>cd infra\ncdk deploy\n</code></pre> </li> </ol>"},{"location":"labs/failover-lab/","title":"Local failover demo","text":"<p>This Failover demonstration is based on a shared store deployment as illustrated in the figure below:</p> <p></p> <p>The local deployment uses the active and standby brokers, and the orchestrator and participant applications communicating with request queue and response queue. </p> <p>The Failover stops the active broker and illustrates the reconnection to standby broker and continuing to produce and consume messages from the two queues, without loosing messages.</p>"},{"location":"labs/failover-lab/#activemq-classic","title":"ActiveMQ Classic","text":"<p>The failover docker compose file is in activeMQ/classic/failover folder and uses the jms-orchestrator and jms-participant applications.</p>"},{"location":"labs/failover-lab/#demonstration","title":"Demonstration","text":"<p>To demonstrate connection failure and automatic reconnect via ActiveMQ failover Transport protocol, we start a long runnning simulation, stop the active broker and let the standby taking the lead. The messages should continue to flow between the two apps. Be sure to have build the producer and consumer images before, if not here are the commands:</p> <ul> <li> <p>Build OCI images:</p> <pre><code># build OCI image for participant and orchestrator apps\nrequest-replyto $ ./buildAdd.sh\n</code></pre> </li> </ul> <p>One script does all</p> <p>The following steps are automated in one script, but you can do step by step too.</p> <pre><code>./testFailover.sh\n</code></pre> <ol> <li> <p>Start the solution with docker compose</p> <pre><code>failover$: docker compose -f e2e-docker-compose.yml up -d\ndocker ps\n\njbcodeforce/jms-classic-orchestrator  8443/tcp, 8778/tcp, 0.0.0.0:8081-&gt;8080/tcp orchestrator\njbcodeforce/jms-classic-participant   8443/tcp, 0.0.0.0:8080-&gt;8080/tcp, 8778/tcp participant\napache/activemq-classic:latest        0.0.0.0:5682-&gt;5672/tcp, 0.0.0.0:8171-&gt;8161/tcp, 0.0.0.0:61626-&gt;61616/tcp   standby\napache/activemq-classic:latest        0.0.0.0:5672-&gt;5672/tcp, 0.0.0.0:8161-&gt;8161/tcp, 0.0.0.0:61616-&gt;61616/tcp   active\n</code></pre> </li> <li> <p>Verify connection of the two apps</p> <pre><code>docker logs participant\ndocker logs orchestrator\n</code></pre> <p>We should see in the logs something like</p> <pre><code># Participant log\n[or.ap.ac.tr.fa.FailoverTransport] (ActiveMQ Task-1) Successfully connected to tcp://active:61616\n# orchestrator log\n[or.ap.ac.tr.fa.FailoverTransport] (ActiveMQ Task-1) Successfully connected to tcp://active:61616\n</code></pre> </li> <li> <p>Verify the queues in the Active broket at http://localhost:8161/admin/queues.jsp, we should have two queues and 1 consumer on each queue.</p> </li> <li> <p>Start long simulation: it starts 100 messages with a small pause of 1 second between message:</p> <pre><code>./e2e/startNorders 100\n</code></pre> </li> <li> <p>Stop the active broker:</p> <pre><code>docker stop active\n</code></pre> </li> <li> <p>Verify queues are visible in the standby admin console http://localhost:8171/</p> <p></p> </li> <li> <p>Verify in both applications logs the reconnection and consumption of messsages are working again: the broker crash around message <code>order_19</code>, the message was received, then the exception happens, followed by the reconnection, and getting new message from Standby broker (<code>order_20</code>).</p> <pre><code>[or.ac.pa.in.ms.OrderMessageParticipantProcessor] (ActiveMQ Session Task-1) Received message: 5c8a9abe-82df-4dff-ab32-c9e18ed17e58,order_19,sku9,460.9405926370402,1,Pending\n[or.ac.pa.in.ms.OrderMessageParticipantProcessor] (ActiveMQ Session Task-1) Reponse sent to replyTo queue {\"messageID\":\"5c8a9abe-82df-4dff-ab32-c9e18ed17e58\",\"orderID\":\"order_19\",\"sku\":\"sku9\",\"price\":460.9405926370402,\"quantity\":1,\"status\":\"assigned\"}\n[or.ap.ac.tr.fa.FailoverTransport] (ActiveMQ Transport: tcp://active/172.23.0.2:61616@36418) Transport (tcp://active:61616) failed, attempting to automatically reconnect: java.io.EOFException\n    at java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\n    at org.apache.activemq.openwire.OpenWireFormat.unmarshal(OpenWireFormat.java:280)\n    at org.apache.activemq.transport.tcp.TcpTransport.readCommand(TcpTransport.java:240)\n    at org.apache.activemq.transport.tcp.TcpTransport.doRun(TcpTransport.java:232)\n    at org.apache.activemq.transport.tcp.TcpTransport.run(TcpTransport.java:215)\n    at java.base/java.lang.Thread.run(Thread.java:829)\n\n[or.ac.pa.in.ms.OrderMessageParticipantProcessor] (ActiveMQ Transport: tcp://active/172.23.0.2:61616@36418) Transport interrupted ... it should recover...\n[or.ac.pa.in.ms.OrderMessageParticipantProcessor] (ActiveMQ Task-3) Transport resumed ... we were right to wait...\n[or.ap.ac.tr.fa.FailoverTransport] (ActiveMQ Task-3) Successfully reconnected to tcp://standby:61626\n[or.ac.pa.in.ms.OrderMessageParticipantProcessor] (ActiveMQ Session Task-1) Received message: 75cbf958-0f78-436d-b277-2807427b2c0e,order_20,sku7,838.9133296950124,2,Pending\n[or.ac.pa.in.ms.OrderMessageParticipantProcessor] (ActiveMQ Session Task-1) Reponse sent to replyTo queue {\"messageID\":\"75cbf958-0f78-436d-b277-2807427b2c0e\",\"orderID\":\"order_20\",\"sku\":\"sku7\",\"price\":838.9133296950124,\"quantity\":2,\"status\":\"assigned\"}\n</code></pre> </li> <li> <p>Stop everything and delete the share folder</p> <pre><code> docker compose -f e2e-docker-compose.yml down\n rm -r data \n</code></pre> </li> </ol> <p>Clean up script</p> <p>The following script stop everything: <pre><code>cleanTheTest.sh\n</code></pre></p>"},{"location":"labs/failover-lab/#configuration-explanation","title":"Configuration explanation","text":"<ul> <li> <p>In the docker compose each app has the failover URL, and hostnames are also defined to docker SDN will resolve to the assigned IP address of each broker:</p> <pre><code>environment:\n  ACTIVEMQ_URL: failover:(tcp://active:61616,tcp://standby:61626)?randomize=false\n</code></pre> </li> <li> <p>Each broker configuration is mounted on the good path inside the broker container, as well as the shared filesystem (data to /tmp/mq/kahadb):</p> <pre><code>volumes:\n  - ./config/broker-1.xml:/opt/apache-activemq/conf/activemq.xml\n  - ./config/jetty.xml:/opt/apache-activemq/conf/jetty.xml\n  - ./data:/tmp/mq/kahadb\n</code></pre> </li> <li> <p>In the broker config which is mapped to the <code>activemq.xml</code> configuration, we find the persistence declaration on the shared filesystem, the network connector between the brokers, and transport connections to accept external apps:</p> <pre><code>     &lt;persistenceAdapter&gt;\n        &lt;kahaDB directory=\"/tmp/mq/kahadb\"/&gt;\n    &lt;/persistenceAdapter&gt;\n    &lt;networkConnectors&gt;\n        &lt;networkConnector uri=\"static:(tcp://standby:61626)\" /&gt;\n    &lt;/networkConnectors&gt;\n    &lt;transportConnectors&gt;\n        &lt;transportConnector name=\"openwire\" uri=\"tcp://0.0.0.0:61616?maximumConnections=1000&amp;amp;wireFormat.maxFrameSize=104857600;updateClusterClients=true\"/&gt;\n</code></pre> <p>The 0.0.0.0 is important, to make the connection, as we run inside container with a docker defined network.</p> </li> </ul>"},{"location":"labs/failover-lab/#activemq-artemis","title":"ActiveMQ Artemis","text":"<p>TO BE TERMINATED</p> <p>The docker image to use is  activeMQ/failover-pt-pt/mq-act-stby-docker-compose.yml. </p> <p>The live broker configuration is in config/broker-1.xml and has the following declaration</p> <pre><code>&lt;ha-policy&gt;\n    &lt;shared-store&gt;\n        &lt;master&gt;\n            &lt;failover-on-shutdown&gt;true&lt;/failover-on-shutdown&gt;\n        &lt;/master&gt;\n    &lt;/shared-store&gt;\n&lt;/ha-policy&gt;\n</code></pre> <p>While the backup server has </p> <pre><code>&lt;ha-policy&gt;\n    &lt;shared-store&gt;\n        &lt;slave&gt;\n            &lt;allow-failback&gt;true&lt;/allow-failback&gt;\n        &lt;/slave&gt;\n    &lt;/shared-store&gt;\n&lt;/ha-policy&gt;\n</code></pre> <p>To test the failover, start the active/passive brokers, and uses two different broker configurations in <code>config</code> folder.</p> <pre><code>docker compose -f mq-act-stby-docker-compose.yml up -d\n# the container names include active and passive\n</code></pre>"},{"location":"labs/ibm-mq/","title":"IBM MQ Labs","text":"<p>This note regroups explanations of the different code samples to run on top of IBM MQ.</p>"},{"location":"labs/ibm-mq/#create-a-ibm-mq-docker-image-for-mac-silicon","title":"Create a IBM MQ docker image for Mac silicon","text":"<p>With MAC M silicon, we need a different docker image, the information to build such image is in this repository, but it is simple, once the repository cloned do: <code>make build-devserver</code>. The created docker image on 11/22/2023 is <code>ibm-mqadvanced-server-dev:9.3.4.0-arm64</code>.</p> <p>The docker compose file in docker-compose for ibm mq can start one instance of IBM MQ broker to be used for development purpose.</p>"},{"location":"labs/ibm-mq/#one-way-point-to-point-code-based-on-jms","title":"One-way point-to-point code based on JMS","text":"<p>For the first demonstration we take a simple JMS producer to send message to IBM MQ queue, <code>DEV.QUEUE.1</code>, consumed by a JMS Consumer App. It is a one-way integration pattern with a point-to-point channel using queue. Nothing fancy, but interesting to see the change to the configuration to work with MQ. Here is the simple diagram:</p> <p></p>"},{"location":"labs/ibm-mq/#developing-the-jms-producer","title":"Developing the JMS producer","text":"<ul> <li>Under pt-to-pt-jms folder, start one IBM MQ Broker with docker: <code>docker-compose -f dev-dc.yaml up -d</code></li> <li>Under <code>jms-producer</code> folder, use <code>quarkus dev</code> to test the app.</li> </ul> <p>Some important notes:</p> <ul> <li>The IBM MQ docker in dev mode (set by environment variable:  <code>MQ_DEV: true</code> in docker compose file), has the following predefined MQ objects: <code>DEV.QUEUE.1</code>, <code>DEV.QUEUE.2</code>, <code>DEV.QUEUE.3</code>, <code>DEV.DEAD.LETTER.QUEUE</code> and the <code>DEV.APP.SVRCONN</code> and <code>DEV.ADMIN.SVRCONN</code> channel. We can use this channel and one of the queue for the demonstration.</li> <li>In the docker compose file, the fact that we configure the env variable <code>MQ_APP_PASSWORD</code> with a password means we need to do the same for the application.</li> <li>The <code>application.properties</code> file for the quarkus app defines user and password to be used to connect, as well as the channel and the queue name:</li> </ul> <pre><code>mq.host=localhost\nmq.port=1414\nmq.channel=DEV.APP.SVRCONN\nmq.qmgr=QM1\nmq.app_user=app\nmq.app_password=passw0rd\nmq.queue_name=DEV.QUEUE.1\n</code></pre> <ul> <li>The connection code has some small modifications as summarized below:</li> </ul> <pre><code>JmsFactoryFactory ff = JmsFactoryFactory.getInstance(WMQConstants.WMQ_PROVIDER);\nconnectionFactory = ff.createConnectionFactory();\nconnectionFactory.setStringProperty(WMQConstants.WMQ_HOST_NAME, this.mqHostname);\nconnectionFactory.setIntProperty(WMQConstants.WMQ_PORT, this.mqHostport);\nconnectionFactory.setStringProperty(WMQConstants.WMQ_CHANNEL, this.mqChannel);\nconnectionFactory.setIntProperty(WMQConstants.WMQ_CONNECTION_MODE, WMQConstants.WMQ_CM_CLIENT);\nconnectionFactory.setStringProperty(WMQConstants.WMQ_QUEUE_MANAGER, this.mqQmgr);\nconnectionFactory.setStringProperty(WMQConstants.WMQ_APPLICATIONNAME, this.appName);\nconnectionFactory.setBooleanProperty(WMQConstants.USER_AUTHENTICATION_MQCSP, false);\nconnectionFactory.setStringProperty(WMQConstants.USERID, this.mqAppUser);\nconnectionFactory.setStringProperty(WMQConstants.PASSWORD, this.mqPassword);\n</code></pre> <p>be sure to add IBM MQ dependency in the pom.xml</p> <pre><code>  &lt;dependency&gt;\n    &lt;groupId&gt;com.ibm.mq&lt;/groupId&gt;\n    &lt;artifactId&gt;com.ibm.mq.allclient&lt;/artifactId&gt;\n    &lt;version&gt;9.3.4.0&lt;/version&gt;\n  &lt;/dependency&gt;\n</code></pre> <p>We will see CCDT parameters later.</p> <ul> <li>Once the app runs, it should be connected with logs showing:</li> </ul> <pre><code>18:41:36 INFO  [or.ac.jm.ProductQuoteProducer] (Quarkus Main Thread) Hostname: localhost\n18:41:36 INFO  [or.ac.jm.ProductQuoteProducer] (Quarkus Main Thread) Port: 1414\n18:41:36 INFO  [or.ac.jm.ProductQuoteProducer] (Quarkus Main Thread) Channel: DEV.APP.SVRCONN\n18:41:36 INFO  [or.ac.jm.ProductQuoteProducer] (Quarkus Main Thread) Qmgr: QM1\n18:41:36 INFO  [or.ac.jm.ProductQuoteProducer] (Quarkus Main Thread) App User: app\n18:41:36 INFO  [or.ac.jm.ProductQuoteProducer] (Quarkus Main Thread) No valid CCDT file detected. Using host, port, and channel properties instead.\n18:41:36 INFO  [or.ac.jm.ProductQuoteProducer] (Quarkus Main Thread) Connect to broker succeed\n18:41:36 INFO  [or.ac.jm.ProductQuoteProducer] (Quarkus Main Thread) JMS Producer Started\n</code></pre> <ul> <li>In the IBM MQ we can see the running channel and the connected application</li> </ul> <p></p> <ul> <li>Posting to the simul REST end point will generate 10 messages:</li> </ul> <pre><code>curl -X 'POST' \\\n'http://localhost:8081/simulator' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '{\n\"delay\": 0,\n\"totalMessageToSend\": 10\n}'\n</code></pre> <ul> <li>The messages are in the queue</li> </ul> <p></p>"},{"location":"labs/ibm-mq/#developing-the-jms-consumer","title":"Developing the JMS consumer","text":"<p>This time the code is in pt-to-pt-jms/jms-consumer. The classical JMS MessageListener implementation with the same code to initiate connection to the server. The code is using Jackson mapper for Json to Java Object for the Quote.</p> <p>The code supports reconnection on exception with some delay to try to reconnect.</p>"},{"location":"labs/ibm-mq/#local-ibm-mq-docker-end-to-end-demo","title":"Local IBM MQ docker end-to-end demo","text":"<ol> <li>Be sure to build the producer and consumer docker images with <code>./buildAll.sh</code> under <code>pt-to-pt-jms</code> folder.</li> <li>Under <code>pt-to-pt-jms</code> start docker compose with one IBM MQ broker: <code>docker-compose up</code>. The trace should demonstrate that IBM MQ will take a little bit more time than the two quarkus apps to start, but those apps are able to reconnect, as they retries every 5 seconds to reconnect to the brokers.</li> <li>Connect to the Console at https://localhost:9443, accept the risk on the non-CA certificate, and use admin/passw0rd to access the console.</li> <li>Connect to the Consumer web page to see the last quote: http://localhost:8080</li> <li>Connect to the Producer swagger UI: http://localhost:8081/q/swagger-ui and start a simulation with 20 records. The trace should list the message produces and consumes.</li> </ol> <pre><code>consumer  | 05:21:43 INFO  [or.ac.jm.in.ms.ProductQuoteConsumer] (DispatchThread: [com.ibm.mq.jmqi.remote.impl.RemoteSession[:/43f01614][connectionId=414D5143514D312020202020202020209A1B686500310040,traceIdentifier=5,remoteTraceIdentifier=8]]) Received message: { \"sku\": sku8,\"quote\": 83 }\nproducer  | 05:21:43 INFO  [or.ac.jm.in.ms.ProductQuoteProducer] (executor-thread-1) Sent: {\"sku\":\"sku8\",\"quote\":83}\nproducer  | 05:21:44 INFO  [or.ac.jm.in.ms.ProductQuoteProducer] (executor-thread-1) Sent: {\"sku\":\"sku1\",\"quote\":69}\nconsumer  | 05:21:44 INFO  [or.ac.jm.in.ms.ProductQuoteConsumer] (DispatchThread: [com.ibm.mq.jmqi.remote.impl.RemoteSession[:/43f01614][connectionId=414D5143514D312020202020202020209A1B686500310040,traceIdentifier=5,remoteTraceIdentifier=8]]) Received message: { \"sku\": sku1,\"quote\": 69 }\nproducer  | 05:21:44 INFO  [or.ac.jm.in.ms.ProductQuoteProducer] (executor-thread-1) Sent: {\"sku\":\"sku7\",\"quote\":18}\nconsumer  | 05:21:44 INFO  [or.ac.jm.in.ms.ProductQuoteConsumer] (DispatchThread: [com.ibm.mq.jmqi.remote.impl.RemoteSession[:/43f01614][connectionId=414D5143514D312020202020202020209A1B686500310040,traceIdentifier=5,remoteTraceIdentifier=8]]) Received message: { \"sku\": sku7,\"quote\": 18 }\nproducer  | 05:21:45 INFO  [or.ac.jm.in.ms.ProductQuoteProducer] (executor-thread-1) Sent: {\"sku\":\"sku10\",\"quote\":94}\nconsumer  | 05:21:45 INFO  [or.ac.jm.in.ms.ProductQuoteConsumer] (DispatchThread: [com.ibm.mq.jmqi.remote.impl.RemoteSession[:/43f01614][connectionId=414D5143514D312020202020202020209A1B686500310040,traceIdentifier=5,remoteTraceIdentifier=8]]) Received message: { \"sku\": sku10,\"quote\": 94 }\nproducer  | 05:21:45 INFO  [or.ac.jm.in.ms.ProductQuoteProducer] (executor-thread-1) Sent: {\"sku\":\"sku6\",\"quote\":59}\nconsumer  | 05:21:45 INFO  [or.ac.jm.in.ms.ProductQuoteConsumer] (DispatchThread: [com.ibm.mq.jmqi.remote.impl.RemoteSession[:/43f01614][connectionId=414D5143514D312020202020202020209A1B686500310040,traceIdentifier=5,remoteTraceIdentifier=8]]) Received message: { \"sku\": sku6,\"quote\": 59 }\n</code></pre>"},{"location":"labs/ibm-mq/#aws-deployment","title":"AWS deployment","text":"<p>We can do a manual deployment or using CDK.</p>"},{"location":"labs/ibm-mq/#installing-docker-on-one-ec2-instance","title":"Installing docker on one EC2 instance","text":"<p>Once creating a t3.micro EC2 instance, SSH to it and then do the following steps:</p> <ul> <li>Install docker, docker compose, start the service, add ec2-user to docker group.</li> </ul> <pre><code>sudo yum install -y docker\nsudo service docker start\nsudo usermod -a -G docker ec2-user\nsudo chkconfig docker on\nsudo curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose\n sudo chmod +x /usr/local/bin/docker-compose\n# logout - login back\ndocker info\ndocker-compose version\n# logout and login again with ec2-user - to avoid the permission denied on docker socket.\ndocker images \n</code></pre>"},{"location":"labs/ibm-mq/#installing-ibm-mq-docker-image-on-ec2","title":"Installing IBM MQ docker image on EC2","text":"<ul> <li>The docker image to use on Linux AMI 2023 is: <code>icr.io/ibm-messaging/mq:latest</code>. </li> </ul> <pre><code>docker pull icr.io/ibm-messaging/mq:latest\n</code></pre> <ul> <li>Create a docker compose file to use this image:</li> </ul> <pre><code>version: '3.7'\nservices:\n  ibmmq:\n    image: icr.io/ibm-messaging/mq:latest\n    docker_name: ibmmq\n    ports:\n        - '1414:1414'\n        - '9443:9443'\n        - '9157:9157'\n    volumes:\n        - qm1data:/mnt/mqm\n    stdin_open: true\n    tty: true\n    restart: always\n    environment:\n        LICENSE: accept\n        MQ_QMGR_NAME: QM1\n        MQ_ADMIN_PASSWORD: passw0rd\n        MQ_APP_PASSWORD: passw0rd\n        MQ_ENABLE_METRICS: true\n        MQ_DEV: true\nvolumes:\n  qm1data:\n</code></pre> <ul> <li> <p>Modify the security group of the EC2 instance to add an inbound rule for custom TCP on port 9443, and another one for TCP port 1414.</p> </li> <li> <p>Start the MQ server</p> </li> </ul> <pre><code>docker-compose up -d\n</code></pre> <ul> <li>Access the IBM Console via the EC2 public URL with port 9443, accept the risk, the user is admin user and password.</li> </ul>"},{"location":"labs/ibm-mq/#deployment-the-apps-in-ecs-fargate","title":"Deployment the apps in ECS Fargate","text":"<p>We can upload the two created docker images to ECR, then defines tasks and service.</p> <ol> <li> <p>First build the docker images for each service using <code>buildAll.sh</code> command: Change the name of the image to adapt to your ECR repository.</p> <pre><code>cd jms-orchestrator\nbuildAll.sh\ndocker push toECR_repository\ncd jms-participant\nbuildAll.sh\ndocker push toECR_repository\n</code></pre> </li> <li> <p>If not already done, use CDK to deploy VPC, Brokers using the instructions here.</p> </li> <li>Deploy Active MQ in active/standby architecture</li> <li>Use CDK to deploy the two apps on ECS Fargate.</li> </ol>"},{"location":"labs/ibm-mq/#solution-cdk-deployment","title":"Solution CDK deployment","text":"<p>The CDK approach deploy an EC2 for IBM MQ broker, the two apps in ECS Fargate.</p>"},{"location":"labs/ibm-mq/#logging","title":"Logging","text":"<p>https://docs.aws.amazon.com/amazon-mq/latest/developer-guide/security-logging-monitoring-cloudwatch.html https://docs.aws.amazon.com/amazon-mq/latest/developer-guide/amazon-mq-accessing-metrics.html</p>"},{"location":"labs/ibm-mq/#useful-source-of-information","title":"Useful source of information","text":"<ul> <li>IBM MQ Developer Essentials</li> <li>MQ Developer Cheat sheet: useful for MQRC_NOT_AUTHORIZED error.</li> <li>Configuring connections between the client and server: </li> </ul>"},{"location":"labs/ow-pt-to-pt-jms/","title":"On-way pattern with Point to Point JMS based producer and consumer","text":"<p>This demonstration is for a one-way integration between a producer app and a consumer app using one queue defined in ActiveMQ. The code is under the activeMQ/ow-pt-to-pt-jms folder.</p>"},{"location":"labs/ow-pt-to-pt-jms/#requirements","title":"Requirements","text":"<ul> <li>Produce message to a queue using JMS protocol, using Java Quarkus.</li> <li>Consumer in a separate app, using JMS</li> </ul>"},{"location":"labs/ow-pt-to-pt-jms/#running-locally","title":"Running Locally","text":"<ul> <li>Build the two docker images for jms-producer and jms-consumer components</li> </ul> <pre><code># under jms-producer\n./buildAll.sh\n# jms-consumer\n./buildAll.sh \n</code></pre> <p>Or use under the <code>ow-pt-to-pt-jms</code> folder the <code>./buildAll.sh</code> command.</p> <ul> <li>Start the one ActiveMQ artemis broker with the the producer and consumer apps:</li> </ul> <pre><code>docker compose up -d\n</code></pre> <ul> <li>The Active MQ console: http://localhost:8161/console.</li> <li> <p>Use the Producer REST API to send one CarRide at Producer home and then the Swagger UI link, using the following json:</p> <pre><code>{\n\"customerID\": \"C01\",\n\"pickup\": \"Location_1\",\n\"destination\": \"Location_2\",\n\"rideDate\": \"11/30/2023\",\n\"rideTime\": \"10:00\",\n\"numberOfPassengers\": 2\n}\n</code></pre> <p></p> <p>Another way is to use curl</p> <pre><code>curl -X 'POST' \\\n'http://localhost:8081/carrides' \\\n-H 'accept: */*' \\\n-H 'Content-Type: application/json' \\\n-d '{\n\"customerID\": \"C01\",\n\"pickup\": \"Location_1\",\n\"destination\": \"Location_2\",\n\"rideDate\": \"11/30/2023\",\n\"rideTime\": \"10:00\",\n\"numberOfPassengers\": 2\n}'\n</code></pre> </li> <li> <p>Once the consumer is running the message appears in the logs: <code>docker logs consumer</code>. Or a call to the REST URL will get the last one received:</p> <pre><code>curl -X 'GET' \\\n'http://localhost:8080/carrides/last' \\\n-H 'accept: text/plain'\n</code></pre> <p>See also the swagger: http://localhost:8080/q/swagger-ui/</p> </li> <li> <p>Finally use the Simulator controller to send n random CarRides</p> <pre><code>curl -X 'POST' \\\n'http://localhost:8081/carrides/simulator' \\\n-H 'accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '{\n\"delay\": 0,\n\"totalMessageToSend\": 10\n}'\n</code></pre> </li> <li> <p>Stop the demo: <code>docker compose down</code></p> </li> </ul>"},{"location":"labs/ow-pt-to-pt-jms/#code-explanation","title":"Code Explanation","text":"<p>The microservice uses the onion architecture from domain-driven design practices with an example of package structure as:</p> <pre><code>\u2514\u2500\u2500 org\n    \u2514\u2500\u2500 acme\n        \u2514\u2500\u2500 jms\n            \u251c\u2500\u2500 infra\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 api\n            \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 CarRideDTO.java\n            \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 CarRideResource.java\n            \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 SimulControl.java\n            \u2502\u00a0\u00a0 \u2514\u2500\u2500 msg\n            \u2502\u00a0\u00a0     \u2514\u2500\u2500 CarRideMsgProducer.java\n            \u2514\u2500\u2500 model\n                \u2514\u2500\u2500 CarRide.java\n</code></pre> <p>The APIs are classical REST resources. The business object is a CarRide to represent booking an autonoumous car ride. The messaging includes specific code for producer, consumer or both in the case pf request/response implementation.</p> <p>As an example, the CarRideProducer is unique in the virtual machine via the ApplicationScoped annotation, and implements ExceptionListener to be able to implement some resilience mechanism in case of broker failover.</p> <pre><code>@ApplicationScoped\npublic class CarRidesProducer implements ExceptionListener{\n</code></pre> <p>As a Quarkus app, properties are defined in <code>src/main/resources/application.properties</code> and injected in the class.</p> <pre><code>    @Inject\n    @ConfigProperty(name=\"queue.name\")\n    public String outQueueName;\n    @Inject\n    @ConfigProperty(name=\"activemq.url\")\n    public String connectionURLs;\n</code></pre> <p>Then the implementation approach is to start to connect to the Broker via JMS as soon as the application is started. (Listen to start event)</p> <pre><code>void onStart(@Observes StartupEvent ev) {\n        try {\n            restablishConnection();\n</code></pre> <p>The connection setup uses classical JMS programming model, with the ConnectionFactory coming from ActiveMQ.</p> <pre><code>private synchronized void restablishConnection() throws JMSException {\n        if (connection == null) {\n            displayParameters();\n            connectionFactory = new ActiveMQConnectionFactory(connectionURLs);\n            connection = connectionFactory.createConnection(user, password);\n            connection.setClientID(\"p-\" + System.currentTimeMillis());\n            connection.setExceptionListener(this);\n        } \n        if (producer == null || producerSession == null) {\n            producerSession = connection.createSession();\n            outQueue = producerSession.createQueue(outQueueName);\n            producer = producerSession.createProducer(outQueue);\n            producer.setTimeToLive(60000); // one minute\n        }\n        connection.start();\n        logger.info(\"Connect to broker succeed\");\n    }\n</code></pre> <p>So the pom.xml needs to includes the Active MQ jars, as well as the jms api jar:</p> <pre><code>    &lt;dependency&gt;\n        &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt;\n        &lt;artifactId&gt;activemq-client&lt;/artifactId&gt;\n        &lt;version&gt;5.18.3&lt;/version&gt;\n    &lt;/dependency&gt;\n</code></pre>"},{"location":"labs/ow-pt-to-pt-jms/#deploy-on-aws","title":"Deploy on AWS","text":"<ul> <li> <p>Create ECR repositories for the two apps after building them:</p> <pre><code>aws ecr create-repository --repository-name j9r/amq-jms-consumer\naws ecr create-repository --repository-name j9r/amq-jms-producer\n</code></pre> </li> <li> <p>Push the two docker images to ECR:</p> <pre><code>export AWS_ACCOUNT_ID=\nexport REGION=us-west-2\naws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com\ndocker tag j9r/amq-jms-consumer:latest $AWS_ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/j9r/amq-jms-consumer:latest\ndocker push $AWS_ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/j9r/amq-jms-consumer:latest\n#\ndocker tag j9r/amq-jms-producer:latest $AWS_ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/j9r/amq-jms-producer:latest\ndocker push $AWS_ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/j9r/amq-jms-producer:latest\n</code></pre> </li> <li> <p>Before creating broker we may assess existing Configurations:</p> <pre><code># First get list of existing configurations\naws mq list-configurations\n# Get the Id of the config relevant to broker.\n</code></pre> </li> <li> <p>If needed we can create a condiguration from an existing broker.xml we have prepared</p> </li> <li> <p>Create Active MQ broker using the command script: in ow-pt-to-pt-jms/IaC/createBroker.sh:</p> <pre><code>export CONFIG_ID=\n</code></pre> </li> </ul>"},{"location":"labs/ow-pt-to-pt-jms/#using-aws-cli","title":"Using AWS CLI","text":"<p>The <code>createBrokers.sh</code> script creates brockers using AWS CLI, and the CLI product document for parameter details.</p>"},{"location":"labs/sqs/","title":"Some basic examples on using SQS","text":""},{"location":"labs/sqs/s3-tenants-async-processing/","title":"Process S3 events for multi-tenant bucket with EDA","text":"Info <p>Created: Dec 2023 - Updated: 01/26/24. State: Draft</p> <p>This study reviews, how to best support the Amazon S3 Event Notification processing, using an event-driven processing approach to help an software vendor computing the storage usage of its users in a multi-tenancy architecture.</p> <p>As in any event-driven solution, the choice of implementation approaches will depend of the requirements (Confucius may have already said it), this article addresses a multi-tenancy use case,  for S3 notification event processing at scale.</p> <p>I cover different messaging technologies like Amazon SQS, SNS, AWS Event Bridge or streaming platform like Kafka.</p> <p>The goal is to help developers or solution architects review the possible solutions and select the most appropriate one to address their own problem. The problem is generic enough to represent a reusable solution.</p>"},{"location":"labs/sqs/s3-tenants-async-processing/#introduction","title":"Introduction","text":"<p>The implementation demonstration in this repository/folder is to mockup a SaaS multi-tenant application using S3 as Data Lake and an asynchronous event-driven processing to address S3 put or update object event notifications with a goal to support 100k+ tenants.</p> <p>Each bucket has top level prefixes assigned to a unique tenant. Within a prefix, user can have a hierarchy of 'folders' and objects. It is a multi-tenancy segregation pattern at the prefix level. The following figure illustrates a simple example of bucket and prefix per tenant organization:</p> <pre><code>tenant-group-1 (bucket)\n\u251c\u2500\u2500 tenant_1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 raw\n\u2502   \u2502   \u251c\u2500\u2500 object_10\n\u2502   \u2502   \u2514\u2500\u2500 object_11\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 silver\n\u2502   \u2502   \u251c\u2500\u2500 object_20\n\u2502   \u2502   \u2514\u2500\u2500 object_21\n\u251c\u2500\u2500 tenant_2\n\u2514\u2500\u2500 tenant_3\n</code></pre> <ul> <li>To control file upload, a tool will take into account the tenant's unique identifier so the system can map the target S3 object prefix to a tenant.</li> <li>The solution needs to support million of files uploaded to S3 per day, at a rate of 200k file moves per minute. Once a file is uploaded (in <code>raw</code> prefix), there are file processors that transform the file into another format (the demonstration in this repository, uses Iceberg) to save in another prefix (<code>silver</code>). The basic processing, for a unique tenant, looks like in following figure:</li> </ul> <p></p> <p>Figure 1: Single tenant processing</p> <ul> <li>The event-driven processor can directly get messages from the SQS queue, using AWS SDK. It has to manage idempotency as S3 Event notification may generate retries, so delivering the same message multiple times.</li> <li>The SaaS platform needs to support hundred of those event-driven processing in parallel, at least one per tenant.</li> <li>The SaaS AWS account is the owner of the buckets, and any queues, or topics used for the solution.</li> </ul>"},{"location":"labs/sqs/s3-tenants-async-processing/#constraints-and-constructs","title":"Constraints and constructs","text":"<ul> <li>An AWS account can have a maximum of 100 buckets per region per account. This is a soft limit.</li> <li>There is no limit on the number of objects stored within a bucket. No limit in the number of prefixes. However, a spike in the API request (on create and update objects) rate might cause throttling. In S3, partitions exist at the prefix level, and not at the object level. S3 doesn't use hierarchy to organize its objects and files. A folder is the value between the two slash (/) characters within the prefix name.</li> <li>S3 supports configuring event notifications on a per-bucket basis, to send events to other AWS services like AWS Lambda, SNS or SQS.</li> <li>As the event-processing job will create new file in another prefix, to avoid looping, the event notification needs to take into account the prefixes. We could use another bucket for the target of the file processing, but to keep tenancy within the constraint of a unique bucket, prefixes are also used.</li> <li>The S3 storage class is <code>Standard</code> or <code>S3 Express one Zone</code>.</li> <li> <p>Each S3 bucket can have up to 100 event notification configurations. A configuration defines the event types to send from S3 and what filters to apply to the event by using the prefix and suffix attributes of the S3 objects:</p> <p></p> <p>Figure 2: Defining S3 event notification in AWS Console</p> <p>With the potential destination to publish the events to:</p> <p></p> <p>Figure 3: Destination for the events </p> </li> <li> <p>Below is an example of Python AWS CDK code to define event notification configuration for ObjectCreated event on bucket <code>tenant-group-1</code>, prefix: <code>tenant-1/raw</code>.</p> <pre><code>queue = sqs.Queue(self, \n                      'tenant-grp-1',\n                      queue_name='tenant-grp-1',\n                      retention_period=Duration.days(1),\n                      encryption=sqs.QueueEncryption.UNENCRYPTED)\n\nbucket = s3.Bucket(self, \"tenant-group-1\",\n                       bucket_name='tenant-group-1',\n                       versioned=False,\n                       removal_policy=RemovalPolicy.DESTROY,\n                       auto_delete_objects=True)\nbucket.add_event_notification(s3.EventType.OBJECT_CREATED, \n                            s3n.SqsDestination(queue),\n                            NotificationKeyFilter( prefix=\"tenant-1/raw\")\n                            )\n</code></pre> </li> <li> <p>Notifications are asynchronous: S3 will queue events and will retry delivery if destinations are unavailable. This avoids blocking the caller.</p> </li> <li>For demonstration purpose, we will process ObjectCreated events. See the other supported notifications for SQS and for the ones in EventBridge. Also CDK is quite static and works well with infrastructure as code and CI/CD practices. For this demonstration SDK may be a better solution to demonstrate the flexibility of the SaaS platform to add tenant dynamically: we can imagine a <code>create tenant</code> API that provisions a prefix and assigns it to an existing bucket dynamically.</li> <li>On rare occasion, S3 retry mechanism might cause duplicate S3 event notification.</li> <li>Event ordering: S3 event notification to SQS FIFO is not supported. Event Notification includes a Sequencer attribute which can be used to determine the order of events for a given object key. Sequencer provides a way to determine the sequence of events. Notifications from events that create objects (PUTs) and delete objects contain a sequencer.</li> </ul>"},{"location":"labs/sqs/s3-tenants-async-processing/#limits","title":"Limits","text":"<ul> <li>Number of bucket per region per account: 100.</li> <li>S3 request performance: 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per partitioned Amazon S3 prefix. So in this example, it will be per tenant. 200k file uploads per minute should create around 3350 events per second.</li> <li>Number of S3 Event Notification: 100 events.</li> <li>S3 event notifications do not natively support event batching.</li> </ul>"},{"location":"labs/sqs/s3-tenants-async-processing/#event-content","title":"Event Content","text":"<p>The S3 Event notification includes metadata about the bucket access done and the object created (not all fields are reported in following json):  </p> <pre><code>{\n    \"Records\": [\n        {\n            \"eventVersion\": \"2.1\",\n            \"eventSource\": \"aws:s3\",\n            \"awsRegion\": \"us-west-2\",\n            \"eventTime\": \"2023-12-22T01:13:20.539Z\",\n            \"eventName\": \"ObjectCreated:Put\",\n            \"userIdentity\": {\n                \"principalId\": \"...\"\n            },\n            \"s3\": {\n                \"s3SchemaVersion\": \"1.0\",\n                \"configurationId\": \".....jU4\",\n                \"bucket\": {\n                    \"name\": \"tenant-group-1\",\n                    \"ownerIdentity\": {\n                        \"principalId\": \"...\"\n                    },\n                    \"arn\": \"arn:aws:s3:::tenantgroup1....ddfgox\"\n                },\n                \"object\": {\n                    \"key\": \"raw/reports.csv\",,\n                    \"eTag\": \"138....79aa\",\n                    \"sequencer\": \"00....99D\"\n                }\n            }\n        }\n    ]\n}\n</code></pre> <p>Example of Python code to access the object's path,</p> <pre><code> for record in events[\"Records\"]:\n    bucket = record[\"s3\"][\"bucket\"]\n    objectName=record[\"s3\"][\"object\"][\"key\"]\n    print(bucket[\"name\"]+\"/\"+objectName)\n</code></pre>"},{"location":"labs/sqs/s3-tenants-async-processing/#potential-solutions","title":"Potential Solutions","text":"<p>In the figure 1 above, we were using SQS queue to support the asynchronous event processing with persistence. As there will be multi-tenant per bucket, we need to fan out the processing per tenant.</p>"},{"location":"labs/sqs/s3-tenants-async-processing/#defining-a-group-of-tenants","title":"Defining a group of tenants","text":"<p>Below is a simple tenant group definition for one S3 bucket and one unique queue:</p> <pre><code>{\n   \"name\": \"tenant-group-1\",\n   \"bucket\": \"&lt;ACCOUNTID&gt;-tenant-group-1\",\n   \"region\": \"us-west-2\",\n   \"queueURL\": \"https://sqs.us-west-2.amazonaws.com/ACCOUNTID/tenant-group-1\",\n   \"queueArn\": \"arn:aws:sqs:us-west-2:ACCOUNTID:tenant-group-1\",\n   \"status\": \"ACTIVE\"\n}\n</code></pre> <p>See the Python code to create tenant group for demonstration purpose.</p> <p>When onboarding a tenant, the platform defines a unique <code>tenant_id</code>, and links it to the target bucket and prefix within a persisted Hash Map. This will be used by the SaaS SDK to put the file in the good <code>bucket / 'folder'</code>. The following json may be persisted in DynamoDB.</p> <pre><code>{\n    \"Name\": {\"S\": \"tenant-1\"},\n    \"RootS3Bucket\": {\"S\": \"40....6-tenant-group-1\"}, \n    \"Region\": {\"S\": \"us-west-2\"}, \n    \"Status\": {\"S\": \"ACTIVE\"}, \n    \"BasePrefix\": {\"S\": \"tenant-1/\"}, \n    \"GroupName\": {\"S\": \"tenant-group-1\"},\n    \"ProcessorURL\": {\"S\": \"https://\"}\n}    \n</code></pre>"},{"location":"labs/sqs/s3-tenants-async-processing/#sqs-only","title":"SQS only","text":"<p>The following approach illustrates the simplest asynchronous architecture, using different S3 event notifications based on prefixes and a unique queue per tenant.</p> <p></p> <p>Figure 5: S3 event notification to SQS tenant queue</p> <ul> <li>There will be a limit of 100, S3 event notifications, per bucket. This seems to be a hard limit. Having 1000 bucket per account per region with 100 event notifications, should support around 100k tenants.</li> <li> <p>Using event notification definition, we can fan-out at the level of the event notification definition, one SQS queue per tenant. Below is a code sample to create the S3 event notification to target a SQS queue, and dedicated per tenant via the prefix name:</p> <pre><code>def eventNotificationPerTenant(tenantName,bucketName,queueArn):\nresponse = s3.put_bucket_notification_configuration(\n            Bucket=bucketName,\n            NotificationConfiguration= {\n                'QueueConfigurations': [\n                {\n                    'QueueArn': queueArn,\n                    'Events': [\n                        's3:ObjectCreated:*'|'s3:ObjectRemoved:*'|'s3:ObjectRestore:*'| 's3:Replication:*'\n                    ],\n                    'Filter': {\n                        'Key': {\n                            'FilterRules': [\n                                {\n                                    'Name': 'prefix',\n                                    'Value': tenantName + \"/raw\"\n                                },\n                            ]\n                        }\n                    }\n                },\n            ]})\n</code></pre> </li> <li> <p>The queue ARN will be the queue per tenant.</p> </li> <li>This approach will scale at the level of the number of event-notification that could be created per bucket.</li> <li> <p>The negative of this approach is the big number of queues and event-notifications to be created when we need to scale at hundred of thousand tenants. The filtering on the file to process will be done by the <code>event-driven transform</code> process.</p> </li> <li> <p>There is no real limit to the number of SQS queue per region per account.</p> </li> <li>Standard queues support a maximum of 120,000 inflight messages (received from the queue but not yet deleted by the consumer). </li> <li>On SQS Standard queue nearly unlimited number of transactions per second per API action.</li> <li>SQS is using at least once delivery semantic.</li> </ul>"},{"location":"labs/sqs/s3-tenants-async-processing/#s3-to-lambda-to-sqs","title":"S3 to Lambda to SQS","text":"<p>A more flexible implementation may use Lambda function as a target to S3 Event Notification, to support flexible routing and filtering implementation, use the fan-out pattern, and support batching the events. If the event-driven processing is exposed via HTTP the Lambda function may directly call the good HTTP endpoint:</p> <p></p> <p>Figure 6: Lambda to fanout to http endpoints</p> <p>Lambda can scale at thousand of instances in parallel. The solution uses one queue per bucket and one S3 event notification definition.</p> <p>When the event-driven processing needs to be asynchronous, then we can add one queue before each event-driven process and the Lambda will send the event to the good queue.</p> <p></p> <p>Figure 7: Lambda to fan out to SQS queues</p> Lambda as a destination to S3 Event Notification <p>The Lambda function can be a target of the S3 event notification as illustrated below:</p> <pre><code>def eventNotificationPerTenantViaLambda(tenantName,bucketName,functionArn):\n    response = s3.put_bucket_notification_configuration(\n        Bucket=bucketName,\n        NotificationConfiguration= {\n        'LambdaFunctionConfigurations': [\n            {\n                'LambdaFunctionArn': functionArn,\n                'Events': [\n                    's3:ObjectCreated:*'|'s3:ObjectRemoved:*'|'s3:ObjectRestore:*'| 's3:Replication:*'|'s3:LifecycleTransition'|'s3:IntelligentTiering'|'s3:ObjectAcl:Put'|'s3:LifecycleExpiration:*'|'s3:LifecycleExpiration:Delete'|'s3:LifecycleExpiration:DeleteMarkerCreated'|'s3:ObjectTagging:*',\n                ],\n                'Filter': {\n                    'Key': {\n                        'FilterRules': [\n                            {\n                            'Name': 'prefix',\n                                'Value': tenantName + \"/raw\"\n                            },\n                        ]\n                    }\n                }\n            },\n            ]\n        },\n        )\n</code></pre> <p>Except that the S3 Event notification is posted to an internal queue in the Lambda service. But this queue is transparent to the developer.</p> <p>For the Lambda routing implementation, Lambda will automatically deletes the message from the queue if it returns a success status.</p> <p>The pricing model is pay per call, duration and memory used.</p> <p>See a proof of concept implementation of the following deployment:</p> <p></p> <p>with demonstration script in this folder.</p>"},{"location":"labs/sqs/s3-tenants-async-processing/#sns-sqs","title":"SNS - SQS","text":"<p>In many Fan-out use cases, one of the solution is to combine SNS with SQS. The S3 event notification target is now a SNS topic.  S3 Event notifications are pushed once in a SNS Service, and the SQS queues are subscribers to the topic and then get the messages.</p> <p></p> <p>The advantages:</p> <ul> <li>Fully decoupled with no data loss as SQS will always listen to SNS topic.</li> <li>SQS adds data persistence, delayed processing and retries of work.</li> <li>Increase the number of subscriber over time.</li> <li>Can use Message Filtering using a JSON policy. But there are limits on the number of subscription filter policies: 200 per topic.</li> <li>Can be used for cross-region delivery, with a SQS queue in another region.</li> </ul> <p>The disadvantages:</p> <ul> <li>There is a limit on the number of filter policies. The solution is to add filtering logic within a Lambda function, which will only be relevant if there are other subscribers to the SNS topic for other type of event processing, like analytics. If there is no other consumer then the S3-&gt; SQS -&gt; Lambda -&gt; SQS solution may be more appropriate.</li> </ul> <p></p>"},{"location":"labs/sqs/s3-tenants-async-processing/#sqs-event-bridge","title":"SQS - Event Bridge","text":"<p>Event Bridge could be a solution if the S3 event processing is exposed with HTTP endpoint. Routing rule will be used to select the target end-point depending of the tenant information. But there is a limit of 2000 rules per event bus, so it may not be a valid solution to address the use case of thousand of tenants.</p> <p></p> <p>Also S3 Event Notifications can be sent to the default event bus only, we may need different buses.</p>"},{"location":"labs/sqs/s3-tenants-async-processing/#sqs-msk","title":"SQS - MSK","text":"<p>Finally Kafka may be a solution to stream the S3 event notification to it, via a SQS queue. The interest of this solution, will be to enable event replay and really embrace an event-driven architecture where any consumers may come to consume messages and use an event-sourcing pattern.</p> <p></p> <p>It may be more complex to deploy as we need to define a MSK cluster, and Kafka Connect - SQS source connector. The Event-driven processes need to consume from Kafka. Event ordering will be kept. It will scale to thousand of consumers. This will be a recommended approach when such events are becoming events that can be consumed by many different consumers.</p>"},{"location":"labs/sqs/s3-tenants-async-processing/#conclusion","title":"Conclusion","text":"<p>For a cost perspective, scale to zero is very important, as the solution deployment does not need to get computers waiting for events to come. The Lambda with SQS queues seems the most appropriate method to address the multi-tenant at scale requirements of this solution. As an alternate approach, using a long term strategy of doing an event-driven architecture then Kafka based solution will be a better choice. </p>"},{"location":"labs/sqs/s3-tenants-async-processing/#sources-of-information","title":"Sources of information","text":"<ul> <li>Product documentation - Amazon S3 Event Notifications</li> <li>Manage event ordering and duplicate events with Amazon S3 Event Notifications - blog</li> <li>Getting visibility into storage usage in multi-tenant Amazon S3 buckets</li> </ul>"},{"location":"labs/sqs/s3-tenants-async-processing/#project-status","title":"Project Status","text":"<p>01/2024:</p> <ul> <li> Python SDK to create a tenant group with one matching SQS queue. Define S3 Event Notification from the bucket to SQS. Keep information of the tenant group in DynamoDB.</li> <li> Python SDK to create a new tenant within a given group: create a prefix under the bucket for raw and silver prefixes. Create a SQS per tenant. Persist information about a tenant in DynamoDB.</li> <li> Python SDK app to write a file to S3 bucket for a tenant.</li> <li> <p> From the tenant_group SQS queue to Lambda event routing function to SQS using SAM</p> </li> <li> <p> S3 event notifications to SNS to Lambda to SQS using SAM</p> </li> <li> CDK to create S3 bucket <code>tenant-grp-2</code> and Event notification to SNS topic <code>tenant-grp-2</code></li> </ul>"}]}